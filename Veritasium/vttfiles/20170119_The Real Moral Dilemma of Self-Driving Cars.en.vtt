WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.720
Push this button.

00:00:02.920 --> 00:00:06.360
It's driving itself. It feels good.

00:00:08.333 --> 00:00:13.233
So, BMW brought me to the Consumer
Electornics Show here in Las Vegas.

00:00:13.233 --> 00:00:15.900
I'm going to check out
the future of driving.

00:00:17.500 --> 00:00:19.400
Did I get it? Am I near?

00:00:19.467 --> 00:00:20.367
[unintelligible]

00:00:20.367 --> 00:00:22.033
Oh! I felt it!

00:00:23.400 --> 00:00:25.633
That really felt like pushing a button.

00:00:25.767 --> 00:00:28.733
In this concept car, there's
a holographic menu screen.

00:00:28.733 --> 00:00:32.000
It works by projecting an
image above this panel.

00:00:32.000 --> 00:00:35.300
And then it uses this camera in the
streering column to determine where

00:00:35.300 --> 00:00:36.267
your finger is.

00:00:36.267 --> 00:00:38.733
And when it detects your
fingers in the right spot,

00:00:38.733 --> 00:00:42.900
it uses ultrasound from these 
speakers to provide haptic feedback -

00:00:42.900 --> 00:00:44.933
you can actually feel it in your fingers.

00:00:45.033 --> 00:00:46.567
It's like a little buzzing.

00:00:47.033 --> 00:00:50.167
But what I really want
to try is NOT driving.

00:00:50.167 --> 00:00:52.167
I can actually talk to the camera.

00:00:52.267 --> 00:00:54.900
Are you sure that this is a good idea?

00:00:54.900 --> 00:00:56.500
So here's a question:

00:00:56.500 --> 00:00:59.067
How much should you
trust an autonomous car?

00:00:59.067 --> 00:01:01.667
This car is now driving itself.

00:01:01.667 --> 00:01:04.833
But I need to be able to
take over at any time.

00:01:04.880 --> 00:01:09.080
I'm still legally reponsible if 
something happens to the car, right?

00:01:09.200 --> 00:01:13.167
But, in the coming years, cars are
going to take over more and more

00:01:13.167 --> 00:01:15.533
of the responsibility for driving safely.

00:01:15.533 --> 00:01:19.133
And that has led a lot of people to
 consider the moral dilemmas faced

00:01:19.133 --> 00:01:21.100
when programming self-driving cars.

00:01:21.100 --> 00:01:25.233
The question is what sort of ethical
framework should we program in

00:01:25.233 --> 00:01:26.667
through autonomous vehicles.

00:01:26.667 --> 00:01:28.767
So it needs to make a decision.

00:01:28.767 --> 00:01:33.500
Swerve left into an SUV or
swerve right into a motorcycle.

00:01:35.000 --> 00:01:39.400
Okay, so we can imagine a lot of weird
situations where an autonomous car

00:01:39.400 --> 00:01:40.667
has to make a tough choice.

00:01:40.667 --> 00:01:44.233
But the real moral dilema is
accidents are happening right now.

00:01:44.233 --> 00:01:48.000
More than 30,000 people are
killed each year in the U.S. alone.

00:01:48.000 --> 00:01:50.233
And more than 2 million are injured.

00:01:50.233 --> 00:01:54.467
And the problem in 94%
of collisions is driver error.

00:01:54.467 --> 00:01:59.400
In 2015, half of all traffic
fatalities occurred on highways.

00:01:59.400 --> 00:02:02.533
So even this level of technology
we've demonstrated today -

00:02:02.533 --> 00:02:07.833
autonomous driving on a highway - 
could save a lot of lives.

00:02:07.840 --> 00:02:11.100
We are already shirking our
responsibility for driving cars.

00:02:11.100 --> 00:02:13.133
We are using our phones.

00:02:13.133 --> 00:02:19.400
In 2014, distracted driving resulted in at
least 3,000 killed 430,000 injured.

00:02:19.400 --> 00:02:23.367
So, if we're not driving, we better 
hope that the tech gets to a level

00:02:23.367 --> 00:02:24.967
where the cars can drive for us.

00:02:24.967 --> 00:02:28.033
My view: this problem is
only going to get worse.

00:02:28.733 --> 00:02:31.800
You know, when elevators
became autonomous,

00:02:31.800 --> 00:02:34.367
a lot of people were
uncomfortable with that.

00:02:34.367 --> 00:02:37.800
They were used to there
being a driver in the elevator,

00:02:37.800 --> 00:02:41.967
so compromises had to be made,
like big red stop buttons

00:02:41.967 --> 00:02:43.367
just to make people comfortable.

00:02:43.367 --> 00:02:47.300
And a nice soothing voice to say 
"Which floor would you like to go to?"

00:02:47.300 --> 00:02:51.133
Now, I know that elevators have many
fewer degrees of freedom

00:02:51.233 --> 00:02:54.867
than a car, but even if you look at
something like airplanes,

00:02:55.300 --> 00:03:00.233
airplanes flying in full autonomous mode
are actually safer - studies show -

00:03:00.233 --> 00:03:02.500
than when pilots can take control.

00:03:02.500 --> 00:03:07.167
I think the moral dilemmas over exactly 
how cars should react in a tiny percentage

00:03:07.167 --> 00:03:09.067
of cases where tough
choices need to be made

00:03:09.067 --> 00:03:11.700
is a distraction from the main problem.

00:03:11.700 --> 00:03:14.667
The longer we wait to get
autonomous vehicles on the road,

00:03:14.667 --> 00:03:16.300
the more people will die.

00:03:16.300 --> 00:03:20.167
And that is the real moral
question of autonomous cars.

00:03:20.167 --> 00:03:22.967
Why aren't we getting
them on the road faster?

00:03:23.067 --> 00:03:24.600
I hope you enjoyed the ride.

00:03:25.467 --> 00:03:26.067
That was cool.

00:03:26.067 --> 00:03:28.500
Now let's head back for the CES.

00:03:28.500 --> 00:03:29.700
Perfect.

