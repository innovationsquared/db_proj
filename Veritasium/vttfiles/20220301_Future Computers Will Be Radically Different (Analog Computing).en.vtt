WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.350
- For hundreds of years,

00:00:01.350 --> 00:00:05.100
analog computers were the most
powerful computers on Earth,

00:00:05.100 --> 00:00:09.720
predicting eclipses, tides,
and guiding anti-aircraft guns.

00:00:09.720 --> 00:00:12.650
Then, with the advent of
solid-state transistors,

00:00:12.650 --> 00:00:14.500
digital computers took off.

00:00:14.500 --> 00:00:18.080
Now, virtually every
computer we use is digital.

00:00:18.080 --> 00:00:21.730
But today, a perfect storm of
factors is setting the scene

00:00:21.730 --> 00:00:24.900
for a resurgence of analog technology.

00:00:24.900 --> 00:00:27.530
This is an analog computer,

00:00:27.530 --> 00:00:30.430
and by connecting these
wires in particular ways,

00:00:30.430 --> 00:00:32.670
I can program it to solve a whole range

00:00:32.670 --> 00:00:34.870
of differential equations.

00:00:34.870 --> 00:00:37.740
For example, this setup
allows me to simulate

00:00:37.740 --> 00:00:40.800
a damped mass oscillating on a spring.

00:00:40.800 --> 00:00:43.720
So on the oscilloscope, you
can actually see the position

00:00:43.720 --> 00:00:45.750
of the mass over time.

00:00:45.750 --> 00:00:48.940
And I can vary the damping,

00:00:48.940 --> 00:00:51.900
or the spring constant,

00:00:51.900 --> 00:00:54.740
or the mass, and we can
see how the amplitude

00:00:54.740 --> 00:00:57.740
and duration of the oscillations change.

00:00:57.740 --> 00:01:00.060
Now what makes this an analog computer

00:01:00.060 --> 00:01:03.440
is that there are no
zeros and ones in here.

00:01:03.440 --> 00:01:06.810
Instead, there's actually
a voltage that oscillates

00:01:06.810 --> 00:01:10.260
up and down exactly
like a mass on a spring.

00:01:10.260 --> 00:01:12.770
The electrical circuitry is an analog

00:01:12.770 --> 00:01:14.330
for the physical problem,

00:01:14.330 --> 00:01:16.660
it just takes place much faster.

00:01:16.660 --> 00:01:19.120
Now, if I change the
electrical connections,

00:01:19.120 --> 00:01:20.300
I can program this computer

00:01:20.300 --> 00:01:22.270
to solve other differential equations,

00:01:22.270 --> 00:01:24.010
like the Lorenz system,

00:01:24.010 --> 00:01:27.290
which is a basic model of
convection in the atmosphere.

00:01:27.290 --> 00:01:29.460
Now the Lorenz system is
famous because it was one

00:01:29.460 --> 00:01:32.270
of the first discovered examples of chaos.

00:01:32.270 --> 00:01:35.520
And here, you can see the Lorenz attractor

00:01:35.520 --> 00:01:38.400
with its beautiful butterfly shape.

00:01:38.400 --> 00:01:39.900
And on this analog computer,

00:01:39.900 --> 00:01:42.360
I can change the parameters

00:01:42.360 --> 00:01:45.563
and see their effects in real time.

00:01:46.410 --> 00:01:47.980
So these examples illustrate some

00:01:47.980 --> 00:01:50.650
of the advantages of analog computers.

00:01:50.650 --> 00:01:53.260
They are incredibly
powerful computing devices,

00:01:53.260 --> 00:01:56.580
and they can complete a
lot of computations fast.

00:01:56.580 --> 00:01:59.133
Plus, they don't take much power to do it.

00:02:01.520 --> 00:02:02.900
With a digital computer,

00:02:02.900 --> 00:02:05.600
if you wanna add two eight-bit numbers,

00:02:05.600 --> 00:02:08.120
you need around 50 transistors,

00:02:08.120 --> 00:02:09.760
whereas with an analog computer,

00:02:09.760 --> 00:02:12.240
you can add two currents,

00:02:12.240 --> 00:02:15.760
just by connecting two wires.

00:02:15.760 --> 00:02:18.380
With a digital computer
to multiply two numbers,

00:02:18.380 --> 00:02:20.970
you need on the order of 1,000 transistors

00:02:20.970 --> 00:02:23.600
all switching zeros and ones,

00:02:23.600 --> 00:02:24.960
whereas with an analog computer,

00:02:24.960 --> 00:02:28.480
you can pass a current through a resistor,

00:02:28.480 --> 00:02:31.850
and then the voltage across this resistor

00:02:31.850 --> 00:02:34.290
will be I times R.

00:02:34.290 --> 00:02:35.700
So effectively,

00:02:35.700 --> 00:02:38.883
you have multiplied two numbers together.

00:02:40.010 --> 00:02:42.930
But analog computers also
have their drawbacks.

00:02:42.930 --> 00:02:43.763
For one thing,

00:02:43.763 --> 00:02:46.430
they are not general-purpose
computing devices.

00:02:46.430 --> 00:02:49.430
I mean, you're not gonna run
Microsoft Word on this thing.

00:02:49.430 --> 00:02:52.990
And also, since the inputs
and outputs are continuous,

00:02:52.990 --> 00:02:55.920
I can't input exact values.

00:02:55.920 --> 00:02:58.980
So if I try to repeat
the same calculation,

00:02:58.980 --> 00:03:01.870
I'm never going to get
the exact same answer.

00:03:01.870 --> 00:03:04.660
Plus, think about
manufacturing analog computers.

00:03:04.660 --> 00:03:06.300
There's always gonna be some variation

00:03:06.300 --> 00:03:08.200
in the exact value of components,

00:03:08.200 --> 00:03:10.370
like resistors or capacitors.

00:03:10.370 --> 00:03:12.260
So as a general rule of thumb,

00:03:12.260 --> 00:03:15.540
you can expect about a 1% error.

00:03:15.540 --> 00:03:17.340
So when you think of analog computers,

00:03:17.340 --> 00:03:20.820
you can think powerful,
fast, and energy-efficient,

00:03:20.820 --> 00:03:25.730
but also single-purpose,
non-repeatable, and inexact.

00:03:25.730 --> 00:03:28.140
And if those sound like deal-breakers,

00:03:28.140 --> 00:03:30.010
it's because they probably are.

00:03:30.010 --> 00:03:31.850
I think these are the major reasons

00:03:31.850 --> 00:03:33.610
why analog computers fell out of favor

00:03:33.610 --> 00:03:36.900
as soon as digital
computers became viable.

00:03:36.900 --> 00:03:41.318
Now, here's why analog computers
may be making a comeback.

00:03:41.318 --> 00:03:43.800
(computers beeping)

00:03:43.800 --> 00:03:46.500
It all starts with
artificial intelligence.

00:03:46.500 --> 00:03:48.710
- [Narrator] A machine
has been programmed to see

00:03:48.710 --> 00:03:50.043
and to move objects.

00:03:51.330 --> 00:03:52.940
- AI isn't new.

00:03:52.940 --> 00:03:55.690
The term was coined back in 1956.

00:03:55.690 --> 00:03:58.760
In 1958, Cornell University psychologist,

00:03:58.760 --> 00:04:01.290
Frank Rosenblatt, built the perceptron,

00:04:01.290 --> 00:04:05.150
designed to mimic how
neurons fire in our brains.

00:04:05.150 --> 00:04:08.930
So here's a basic model of how
neurons in our brains work.

00:04:08.930 --> 00:04:12.160
An individual neuron
can either fire or not,

00:04:12.160 --> 00:04:14.380
so its level of activation
can be represented

00:04:14.380 --> 00:04:16.480
as a one or a zero.

00:04:16.480 --> 00:04:18.370
The input to one neuron

00:04:18.370 --> 00:04:21.130
is the output from a bunch other neurons,

00:04:21.130 --> 00:04:22.820
but the strength of these connections

00:04:22.820 --> 00:04:24.410
between neurons varies,

00:04:24.410 --> 00:04:27.420
so each one can be given
a different weight.

00:04:27.420 --> 00:04:29.410
Some connections are excitatory,

00:04:29.410 --> 00:04:30.910
so they have positive weights,

00:04:30.910 --> 00:04:32.840
while others are inhibitory,

00:04:32.840 --> 00:04:34.550
so they have negative weights.

00:04:34.550 --> 00:04:35.470
And the way to figure out

00:04:35.470 --> 00:04:37.670
whether a particular neuron fires,

00:04:37.670 --> 00:04:40.380
is to take the activation
of each input neuron

00:04:40.380 --> 00:04:42.600
and multiply by its weight,

00:04:42.600 --> 00:04:44.370
and then add these all together.

00:04:44.370 --> 00:04:47.490
If their sum is greater than
some number called the bias,

00:04:47.490 --> 00:04:49.050
then the neuron fires,

00:04:49.050 --> 00:04:51.963
but if it's less than that,
the neuron doesn't fire.

00:04:53.460 --> 00:04:57.190
As input, Rosenblatt's
perceptron had 400 photocells

00:04:57.190 --> 00:04:59.080
arranged in a square grid,

00:04:59.080 --> 00:05:02.160
to capture a 20 by 20-pixel image.

00:05:02.160 --> 00:05:04.670
You can think of each
pixel as an input neuron,

00:05:04.670 --> 00:05:07.780
with its activation being
the brightness of the pixel.

00:05:07.780 --> 00:05:09.020
Although strictly speaking,

00:05:09.020 --> 00:05:11.910
the activation should
be either zero or one,

00:05:11.910 --> 00:05:15.960
we can let it take any
value between zero and one.

00:05:15.960 --> 00:05:18.090
All of these neurons are connected

00:05:18.090 --> 00:05:20.130
to a single output neuron,

00:05:20.130 --> 00:05:23.140
each via its own adjustable weight.

00:05:23.140 --> 00:05:25.370
So to see if the output neuron will fire,

00:05:25.370 --> 00:05:28.840
you multiply the activation
of each neuron by its weight,

00:05:28.840 --> 00:05:30.440
and add them together.

00:05:30.440 --> 00:05:33.260
This is essentially a vector dot product.

00:05:33.260 --> 00:05:36.600
If the answer is larger than
the bias, the neuron fires,

00:05:36.600 --> 00:05:38.890
and if not, it doesn't.

00:05:38.890 --> 00:05:40.590
Now the goal of the perceptron

00:05:40.590 --> 00:05:43.690
was to reliably distinguish
between two images,

00:05:43.690 --> 00:05:45.970
like a rectangle and a circle.

00:05:45.970 --> 00:05:46.803
For example,

00:05:46.803 --> 00:05:48.600
the output neuron could always fire

00:05:48.600 --> 00:05:49.950
when presented with a circle,

00:05:49.950 --> 00:05:52.930
but never when presented with a rectangle.

00:05:52.930 --> 00:05:55.890
To achieve this, the
perception had to be trained,

00:05:55.890 --> 00:05:58.410
that is, shown a series
of different circles

00:05:58.410 --> 00:06:02.490
and rectangles, and have its
weights adjusted accordingly.

00:06:02.490 --> 00:06:05.350
We can visualize the weights as an image,

00:06:05.350 --> 00:06:09.440
since there's a unique weight
for each pixel of the image.

00:06:09.440 --> 00:06:12.470
Initially, Rosenblatt set
all the weights to zero.

00:06:12.470 --> 00:06:14.530
If the perceptron's output is correct,

00:06:14.530 --> 00:06:16.820
for example, here it's shown a rectangle

00:06:16.820 --> 00:06:19.000
and the output neuron doesn't fire,

00:06:19.000 --> 00:06:21.260
no change is made to the weights.

00:06:21.260 --> 00:06:23.910
But if it's wrong, then
the weights are adjusted.

00:06:23.910 --> 00:06:25.920
The algorithm for updating the weights

00:06:25.920 --> 00:06:27.640
is remarkably simple.

00:06:27.640 --> 00:06:30.730
Here, the output neuron didn't
fire when it was supposed to

00:06:30.730 --> 00:06:32.370
because it was shown a circle.

00:06:32.370 --> 00:06:33.890
So to modify the weights,

00:06:33.890 --> 00:06:38.210
you simply add the input
activations to the weights.

00:06:38.210 --> 00:06:40.540
If the output neuron
fires when it shouldn't,

00:06:40.540 --> 00:06:42.890
like here, when shown a rectangle,

00:06:42.890 --> 00:06:45.670
well, then you subtract
the input activations

00:06:45.670 --> 00:06:48.280
from the weights, and you keep doing this

00:06:48.280 --> 00:06:50.840
until the perceptron correctly identifies

00:06:50.840 --> 00:06:52.630
all the training images.

00:06:52.630 --> 00:06:55.520
It was shown that this
algorithm will always converge,

00:06:55.520 --> 00:06:58.180
so long as it's possible
to map the two categories

00:06:58.180 --> 00:07:00.035
into distinct groups.

00:07:00.035 --> 00:07:02.240
(footsteps thumping)

00:07:02.240 --> 00:07:04.960
The perceptron was
capable of distinguishing

00:07:04.960 --> 00:07:07.900
between different shapes,
like rectangles and triangles,

00:07:07.900 --> 00:07:09.220
or between different letters.

00:07:09.220 --> 00:07:10.650
And according to Rosenblatt,

00:07:10.650 --> 00:07:14.020
it could even tell the
difference between cats and dogs.

00:07:14.020 --> 00:07:15.910
He said the machine was capable

00:07:15.910 --> 00:07:18.940
of what amounts to original thought,

00:07:18.940 --> 00:07:20.880
and the media lapped it up.

00:07:20.880 --> 00:07:22.887
The "New York Times" called the perceptron

00:07:22.887 --> 00:07:25.280
"the embryo of an electronic computer

00:07:25.280 --> 00:07:28.350
that the Navy expects will
be able to walk, talk,

00:07:28.350 --> 00:07:30.890
see, write, reproduce itself,

00:07:30.890 --> 00:07:33.647
and be conscious of its existence."

00:07:34.750 --> 00:07:36.950
- [Narrator] After training
on lots of examples,

00:07:36.950 --> 00:07:39.533
it's given new faces it has never seen,

00:07:39.533 --> 00:07:43.480
and is able to successfully
distinguish male from female.

00:07:43.480 --> 00:07:45.020
It has learned.

00:07:45.020 --> 00:07:47.530
- In reality, the perceptron
was pretty limited

00:07:47.530 --> 00:07:48.730
in what it could do.

00:07:48.730 --> 00:07:52.050
It could not, in fact,
tell apart dogs from cats.

00:07:52.050 --> 00:07:53.950
This and other critiques were raised

00:07:53.950 --> 00:07:58.187
in a book by MIT giants,
Minsky and Papert, in 1969.

00:07:58.187 --> 00:08:00.330
And that led to a bust period

00:08:00.330 --> 00:08:03.530
for artificial neural
networks and AI in general.

00:08:03.530 --> 00:08:06.720
It's known as the first AI winter.

00:08:06.720 --> 00:08:09.370
Rosenblatt did not survive this winter.

00:08:09.370 --> 00:08:12.200
He drowned while sailing in Chesapeake Bay

00:08:12.200 --> 00:08:14.118
on his 43rd birthday.

00:08:14.118 --> 00:08:17.330
(mellow upbeat music)

00:08:17.330 --> 00:08:19.660
- [Narrator] The NAV Lab
is a road-worthy truck,

00:08:19.660 --> 00:08:22.290
modified so that researchers or computers

00:08:22.290 --> 00:08:25.360
can control the vehicle
as occasion demands.

00:08:25.360 --> 00:08:28.090
- [Derek] In the 1980s,
there was an AI resurgence

00:08:28.090 --> 00:08:30.090
when researchers at
Carnegie Mellon created one

00:08:30.090 --> 00:08:32.470
of the first self-driving cars.

00:08:32.470 --> 00:08:33.860
The vehicle was steered

00:08:33.860 --> 00:08:36.900
by an artificial neural
network called ALVINN.

00:08:36.900 --> 00:08:37.950
It was similar to the perceptron,

00:08:37.950 --> 00:08:41.080
except it had a hidden
layer of artificial neurons

00:08:41.080 --> 00:08:43.300
between the input and output.

00:08:43.300 --> 00:08:47.060
As input, ALVINN received
30 by 32-pixel images

00:08:47.060 --> 00:08:48.400
of the road ahead.

00:08:48.400 --> 00:08:51.600
Here, I'm showing them as 60 by 64 pixels.

00:08:51.600 --> 00:08:54.020
But each of these input
neurons was connected

00:08:54.020 --> 00:08:57.950
via an adjustable weight to a
hidden layer of four neurons.

00:08:57.950 --> 00:09:01.600
These were each connected
to 32 output neurons.

00:09:01.600 --> 00:09:04.180
So to go from one layer of
the network to the next,

00:09:04.180 --> 00:09:06.860
you perform a matrix multiplication:

00:09:06.860 --> 00:09:10.120
the input activation times the weights.

00:09:10.120 --> 00:09:12.580
The output neuron with
the greatest activation

00:09:12.580 --> 00:09:14.483
determines the steering angle.

00:09:15.540 --> 00:09:16.920
To train the neural net,

00:09:16.920 --> 00:09:18.550
a human drove the vehicle,

00:09:18.550 --> 00:09:20.900
providing the correct steering angle

00:09:20.900 --> 00:09:22.720
for a given input image.

00:09:22.720 --> 00:09:24.810
All the weights in the
neural network were adjusted

00:09:24.810 --> 00:09:25.643
through the training

00:09:25.643 --> 00:09:27.850
so that ALVINN's output
better matched that

00:09:27.850 --> 00:09:29.063
of the human driver.

00:09:30.270 --> 00:09:31.780
The method for adjusting the weights

00:09:31.780 --> 00:09:33.390
is called backpropagation,

00:09:33.390 --> 00:09:34.930
which I won't go into here,

00:09:34.930 --> 00:09:37.350
but Welch Labs has a great series on this,

00:09:37.350 --> 00:09:39.253
which I'll link to in the description.

00:09:40.090 --> 00:09:41.900
Again, you can visualize the weights

00:09:41.900 --> 00:09:44.500
for the four hidden neurons as images.

00:09:44.500 --> 00:09:46.720
The weights are initially
set to be random,

00:09:46.720 --> 00:09:48.210
but as training progresses,

00:09:48.210 --> 00:09:51.760
the computer learns to pick
up on certain patterns.

00:09:51.760 --> 00:09:54.890
You can see the road markings
emerge in the weights.

00:09:54.890 --> 00:09:58.190
Simultaneously, the output
steering angle coalesces

00:09:58.190 --> 00:10:00.620
onto the human steering angle.

00:10:00.620 --> 00:10:03.080
The computer drove the
vehicle at a top speed

00:10:03.080 --> 00:10:06.350
of around one or two kilometers per hour.

00:10:06.350 --> 00:10:07.830
It was limited by the speed

00:10:07.830 --> 00:10:10.763
at which the computer could
perform matrix multiplication.

00:10:12.250 --> 00:10:13.700
Despite these advances,

00:10:13.700 --> 00:10:15.810
artificial neural networks still struggled

00:10:15.810 --> 00:10:17.550
with seemingly simple tasks,

00:10:17.550 --> 00:10:19.920
like telling apart cats and dogs.

00:10:19.920 --> 00:10:22.250
And no one knew whether hardware

00:10:22.250 --> 00:10:24.210
or software was the weak link.

00:10:24.210 --> 00:10:26.560
I mean, did we have a good
model of intelligence,

00:10:26.560 --> 00:10:28.590
we just needed more computer power?

00:10:28.590 --> 00:10:30.470
Or, did we have the wrong idea

00:10:30.470 --> 00:10:33.550
about how to make intelligence
systems altogether?

00:10:33.550 --> 00:10:36.140
So artificial intelligence
experienced another lull

00:10:36.140 --> 00:10:38.150
in the 1990s.

00:10:38.150 --> 00:10:39.440
By the mid 2000s,

00:10:39.440 --> 00:10:43.250
most AI researchers were
focused on improving algorithms.

00:10:43.250 --> 00:10:45.870
But one researcher, Fei-Fei Li,

00:10:45.870 --> 00:10:48.260
thought maybe there was
a different problem.

00:10:48.260 --> 00:10:50.350
Maybe these artificial neural networks

00:10:50.350 --> 00:10:52.540
just needed more data to train on.

00:10:52.540 --> 00:10:56.170
So she planned to map out
the entire world of objects.

00:10:56.170 --> 00:10:59.220
From 2006 to 2009, she created ImageNet,

00:10:59.220 --> 00:11:02.550
a database of 1.2 million
human-labeled images,

00:11:02.550 --> 00:11:03.383
which at the time,

00:11:03.383 --> 00:11:06.330
was the largest labeled image
dataset ever constructed.

00:11:06.330 --> 00:11:08.258
And from 2010 to 2017,

00:11:08.258 --> 00:11:10.400
ImageNet ran an annual contest:

00:11:10.400 --> 00:11:13.740
the ImageNet Large Scale
Visual Recognition Challenge,

00:11:13.740 --> 00:11:16.380
where software programs
competed to correctly detect

00:11:16.380 --> 00:11:17.970
and classify images.

00:11:17.970 --> 00:11:21.110
Images were classified into
1,000 different categories,

00:11:21.110 --> 00:11:23.520
including 90 different dog breeds.

00:11:23.520 --> 00:11:25.460
A neural network competing
in this competition

00:11:25.460 --> 00:11:28.220
would have an output
layer of 1,000 neurons,

00:11:28.220 --> 00:11:30.460
each corresponding to a category of object

00:11:30.460 --> 00:11:32.230
that could appear in the image.

00:11:32.230 --> 00:11:34.500
If the image contains,
say, a German shepherd,

00:11:34.500 --> 00:11:37.430
then the output neuron
corresponding to German shepherd

00:11:37.430 --> 00:11:39.770
should have the highest activation.

00:11:39.770 --> 00:11:43.120
Unsurprisingly, it turned
out to be a tough challenge.

00:11:43.120 --> 00:11:45.130
One way to judge the performance of an AI

00:11:45.130 --> 00:11:48.360
is to see how often the five
highest neuron activations

00:11:48.360 --> 00:11:50.920
do not include the correct category.

00:11:50.920 --> 00:11:53.840
This is the so-called top-5 error rate.

00:11:53.840 --> 00:11:56.920
In 2010, the best performer
had a top-5 error rate

00:11:56.920 --> 00:12:01.080
of 28.2%, meaning that
nearly 1/3 of the time,

00:12:01.080 --> 00:12:04.570
the correct answer was not
among its top five guesses.

00:12:04.570 --> 00:12:09.270
In 2011, the error rate of
the best performer was 25.8%,

00:12:09.270 --> 00:12:11.360
a substantial improvement.

00:12:11.360 --> 00:12:12.370
But the next year,

00:12:12.370 --> 00:12:13.620
an artificial neural network

00:12:13.620 --> 00:12:16.220
from the University of
Toronto, called AlexNet,

00:12:16.220 --> 00:12:17.870
blew away the competition

00:12:17.870 --> 00:12:22.410
with a top-5 error rate of just 16.4%.

00:12:22.410 --> 00:12:25.840
What set AlexNet apart
was its size and depth.

00:12:25.840 --> 00:12:27.720
The network consisted of eight layers,

00:12:27.720 --> 00:12:30.700
and in total, 500,000 neurons.

00:12:30.700 --> 00:12:31.533
To train AlexNet,

00:12:31.533 --> 00:12:35.580
60 million weights and biases
had to be carefully adjusted

00:12:35.580 --> 00:12:37.500
using the training database.

00:12:37.500 --> 00:12:40.040
Because of all the big
matrix multiplications,

00:12:40.040 --> 00:12:43.570
processing a single image
required 700 million

00:12:43.570 --> 00:12:45.330
individual math operations.

00:12:45.330 --> 00:12:48.280
So training was computationally intensive.

00:12:48.280 --> 00:12:51.330
The team managed it by
pioneering the use of GPUs,

00:12:51.330 --> 00:12:52.910
graphical processing units,

00:12:52.910 --> 00:12:56.330
which are traditionally used
for driving displays, screens.

00:12:56.330 --> 00:13:00.110
So they're specialized for
fast parallel computations.

00:13:00.110 --> 00:13:02.520
The AlexNet paper
describing their research

00:13:02.520 --> 00:13:04.260
is a blockbuster.

00:13:04.260 --> 00:13:07.680
It's now been cited over 100,000 times,

00:13:07.680 --> 00:13:10.390
and it identifies the
scale of the neural network

00:13:10.390 --> 00:13:12.980
as key to its success.

00:13:12.980 --> 00:13:16.160
It takes a lot of computation
to train and run the network,

00:13:16.160 --> 00:13:19.290
but the improvement in
performance is worth it.

00:13:19.290 --> 00:13:20.740
With others following their lead,

00:13:20.740 --> 00:13:22.010
the top-5 error rate

00:13:22.010 --> 00:13:23.900
on the ImageNet competition plummeted

00:13:23.900 --> 00:13:28.210
in the years that followed,
down to 3.6% in 2015.

00:13:28.210 --> 00:13:31.160
That is better than human performance.

00:13:31.160 --> 00:13:32.730
The neural network that achieved this

00:13:32.730 --> 00:13:35.030
had 100 layers of neurons.

00:13:35.030 --> 00:13:36.490
So the future is clear:

00:13:36.490 --> 00:13:38.390
We will see ever increasing demand

00:13:38.390 --> 00:13:40.820
for ever larger neural networks.

00:13:40.820 --> 00:13:43.040
And this is a problem for several reasons:

00:13:43.040 --> 00:13:45.150
One is energy consumption.

00:13:45.150 --> 00:13:47.060
Training a neural network
requires an amount

00:13:47.060 --> 00:13:49.460
of electricity similar
to the yearly consumption

00:13:49.460 --> 00:13:50.990
of three households.

00:13:50.990 --> 00:13:54.100
Another issue is the so-called
Von Neumann Bottleneck.

00:13:54.100 --> 00:13:55.870
Virtually every modern digital computer

00:13:55.870 --> 00:13:57.200
stores data in memory,

00:13:57.200 --> 00:14:00.360
and then accesses it as needed over a bus.

00:14:00.360 --> 00:14:02.840
When performing the huge
matrix multiplications required

00:14:02.840 --> 00:14:04.230
by deep neural networks,

00:14:04.230 --> 00:14:05.670
most of the time and energy goes

00:14:05.670 --> 00:14:07.960
into fetching those weight values rather

00:14:07.960 --> 00:14:10.490
than actually doing the computation.

00:14:10.490 --> 00:14:13.250
And finally, there are the
limitations of Moore's Law.

00:14:13.250 --> 00:14:14.960
For decades, the number of transistors

00:14:14.960 --> 00:14:18.190
on a chip has been doubling
approximately every two years,

00:14:18.190 --> 00:14:20.130
but now the size of a transistor

00:14:20.130 --> 00:14:21.910
is approaching the size of an atom.

00:14:21.910 --> 00:14:24.690
So there are some fundamental
physical challenges

00:14:24.690 --> 00:14:26.880
to further miniaturization.

00:14:26.880 --> 00:14:30.270
So this is the perfect
storm for analog computers.

00:14:30.270 --> 00:14:32.640
Digital computers are
reaching their limits.

00:14:32.640 --> 00:14:35.880
Meanwhile, neural networks
are exploding in popularity,

00:14:35.880 --> 00:14:38.000
and a lot of what they do boils down

00:14:38.000 --> 00:14:41.350
to a single task: matrix multiplication.

00:14:41.350 --> 00:14:44.070
Best of all, neural networks
don't need the precision

00:14:44.070 --> 00:14:45.350
of digital computers.

00:14:45.350 --> 00:14:48.990
Whether the neural net
is 96% or 98% confident

00:14:48.990 --> 00:14:50.410
the image contains a chicken,

00:14:50.410 --> 00:14:52.810
it doesn't really matter,
it's still a chicken.

00:14:52.810 --> 00:14:54.870
So slight variability in components

00:14:54.870 --> 00:14:57.372
or conditions can be tolerated.

00:14:57.372 --> 00:14:58.810
(upbeat rock music)

00:14:58.810 --> 00:15:01.370
I went to an analog
computing startup in Texas,

00:15:01.370 --> 00:15:03.370
called Mythic AI.

00:15:03.370 --> 00:15:06.820
Here, they're creating analog
chips to run neural networks.

00:15:06.820 --> 00:15:09.923
And they demonstrated
several AI algorithms for me.

00:15:10.980 --> 00:15:11.813
- Oh, there you go.

00:15:11.813 --> 00:15:13.290
See, it's getting you.
(Derek laughs)

00:15:13.290 --> 00:15:14.770
Yeah.
- That's fascinating.

00:15:14.770 --> 00:15:17.630
- The biggest use case is
augmented in virtual reality.

00:15:17.630 --> 00:15:19.150
If your friend is in a different,

00:15:19.150 --> 00:15:20.710
they're at their house
and you're at your house,

00:15:20.710 --> 00:15:24.120
you can actually render each
other in the virtual world.

00:15:24.120 --> 00:15:27.280
So it needs to really
quickly capture your pose,

00:15:27.280 --> 00:15:29.350
and then render it in the VR world.

00:15:29.350 --> 00:15:31.333
- So, hang on, is this
for the metaverse thing?

00:15:31.333 --> 00:15:35.640
- Yeah, this is a very
metaverse application.

00:15:35.640 --> 00:15:38.630
This is depth estimation
from just a single webcam.

00:15:38.630 --> 00:15:39.950
It's just taking this scene,

00:15:39.950 --> 00:15:41.360
and then it's doing a heat map.

00:15:41.360 --> 00:15:43.750
So if it's bright, it means it's close.

00:15:43.750 --> 00:15:45.980
And if it's far away, it makes it black.

00:15:45.980 --> 00:15:47.560
- [Derek] Now all these
algorithms can be run

00:15:47.560 --> 00:15:48.850
on digital computers,

00:15:48.850 --> 00:15:52.250
but here, the matrix multiplication
is actually taking place

00:15:52.250 --> 00:15:54.420
in the analog domain.
(light music)

00:15:54.420 --> 00:15:55.800
To make this possible,

00:15:55.800 --> 00:15:59.490
Mythic has repurposed
digital flash storage cells.

00:15:59.490 --> 00:16:01.210
Normally these are used as memory

00:16:01.210 --> 00:16:03.630
to store either a one or a zero.

00:16:03.630 --> 00:16:07.460
If you apply a large positive
voltage to the control gate,

00:16:07.460 --> 00:16:10.210
electrons tunnel up through
an insulating barrier

00:16:10.210 --> 00:16:12.440
and become trapped on the floating gate.

00:16:12.440 --> 00:16:13.570
Remove the voltage,

00:16:13.570 --> 00:16:15.440
and the electrons can
remain on the floating gate

00:16:15.440 --> 00:16:18.609
for decades, preventing the
cell from conducting current.

00:16:18.609 --> 00:16:21.320
And that's how you can store
either a one or a zero.

00:16:21.320 --> 00:16:22.960
You can read out the stored value

00:16:22.960 --> 00:16:25.060
by applying a small voltage.

00:16:25.060 --> 00:16:26.970
If there are electrons
on the floating gate,

00:16:26.970 --> 00:16:29.580
no current flows, so that's a zero.

00:16:29.580 --> 00:16:30.940
If there aren't electrons,

00:16:30.940 --> 00:16:33.920
then current does flow, and that's a one.

00:16:33.920 --> 00:16:36.030
Now Mythic's idea is to use these cells

00:16:36.030 --> 00:16:40.000
not as on/off switches,
but as variable resistors.

00:16:40.000 --> 00:16:42.900
They do this by putting a
specific number of electrons

00:16:42.900 --> 00:16:45.670
on each floating gate,
instead of all or nothing.

00:16:45.670 --> 00:16:47.330
The greater the number of electrons,

00:16:47.330 --> 00:16:49.810
the higher the resistance of the channel.

00:16:49.810 --> 00:16:52.000
When you later apply a small voltage,

00:16:52.000 --> 00:16:55.720
the current that flows
is equal to V over R.

00:16:55.720 --> 00:16:59.250
But you can also think of this
as voltage times conductance,

00:16:59.250 --> 00:17:02.510
where conductance is just
the reciprocal of resistance.

00:17:02.510 --> 00:17:04.470
So a single flash cell can be used

00:17:04.470 --> 00:17:09.130
to multiply two values together,
voltage times conductance.

00:17:09.130 --> 00:17:11.860
So to use this to run an
artificial neural network,

00:17:11.860 --> 00:17:14.750
well they first write all the
weights to the flash cells

00:17:14.750 --> 00:17:16.870
as each cell's conductance.

00:17:16.870 --> 00:17:19.270
Then, they input the activation values

00:17:19.270 --> 00:17:21.490
as the voltage on the cells.

00:17:21.490 --> 00:17:23.630
And the resulting current is the product

00:17:23.630 --> 00:17:25.500
of voltage times conductance,

00:17:25.500 --> 00:17:28.300
which is activation times weight.

00:17:28.300 --> 00:17:30.860
The cells are wired together in such a way

00:17:30.860 --> 00:17:34.000
that the current from each
multiplication adds together,

00:17:34.000 --> 00:17:36.607
completing the matrix multiplication.

00:17:36.607 --> 00:17:39.040
(light music)

00:17:39.040 --> 00:17:40.920
- So this is our first product.

00:17:40.920 --> 00:17:45.860
This can do 25 trillion
math operations per second.

00:17:45.860 --> 00:17:47.040
- [Derek] 25 trillion.

00:17:47.040 --> 00:17:49.400
- Yep, 25 trillion math
operations per second,

00:17:49.400 --> 00:17:50.540
in this little chip here,

00:17:50.540 --> 00:17:52.620
burning about three watts of power.

00:17:52.620 --> 00:17:54.930
- [Derek] How does it
compare to a digital chip?

00:17:54.930 --> 00:17:57.900
- The newer digital
systems can do anywhere

00:17:57.900 --> 00:18:00.680
from 25 to 100 trillion
operations per second,

00:18:00.680 --> 00:18:02.780
but they are big, thousand-dollar systems

00:18:02.780 --> 00:18:06.590
that are spitting out 50
to 100 watts of power.

00:18:06.590 --> 00:18:07.680
- [Derek] Obviously this isn't

00:18:07.680 --> 00:18:09.460
like an apples apples comparison, right?

00:18:09.460 --> 00:18:10.580
- No, it's not apples to apples.

00:18:10.580 --> 00:18:13.660
I mean, training those algorithms,

00:18:13.660 --> 00:18:15.510
you need big hardware like this.

00:18:15.510 --> 00:18:17.610
You can just do all sorts
of stuff on the GPU,

00:18:17.610 --> 00:18:20.360
but if you specifically
are doing AI workloads

00:18:20.360 --> 00:18:22.720
and you wanna deploy 'em,
you could use this instead.

00:18:22.720 --> 00:18:25.170
You can imagine them in security cameras,

00:18:25.170 --> 00:18:26.690
autonomous systems,

00:18:26.690 --> 00:18:29.120
inspection equipment for manufacturing.

00:18:29.120 --> 00:18:30.880
Every time they make a Frito-Lay chip,

00:18:30.880 --> 00:18:32.200
they inspect it with a camera,

00:18:32.200 --> 00:18:36.170
and the bad Fritos get blown
off of the conveyor belt.

00:18:36.170 --> 00:18:37.750
But they're using artificial intelligence

00:18:37.750 --> 00:18:40.410
to spot which Fritos are good and bad.

00:18:40.410 --> 00:18:42.650
- Some have proposed
using analog circuitry

00:18:42.650 --> 00:18:43.920
in smart home speakers,

00:18:43.920 --> 00:18:47.650
solely to listen for the wake
word, like Alexa or Siri.

00:18:47.650 --> 00:18:49.940
They would use a lot less
power and be able to quickly

00:18:49.940 --> 00:18:53.310
and reliably turn on the
digital circuitry of the device.

00:18:53.310 --> 00:18:56.420
But you still have to deal
with the challenges of analog.

00:18:56.420 --> 00:18:58.120
- So for one of the popular networks,

00:18:58.120 --> 00:19:00.740
there would be 50 sequences

00:19:00.740 --> 00:19:02.670
of matrix multiplies that you're doing.

00:19:02.670 --> 00:19:05.040
Now, if you did that entirely
in the analog domain,

00:19:05.040 --> 00:19:06.340
by the time it gets to the output,

00:19:06.340 --> 00:19:07.810
it's just so distorted

00:19:07.810 --> 00:19:10.260
that you don't have any result at all.

00:19:10.260 --> 00:19:12.130
So you convert it from the analog domain,

00:19:12.130 --> 00:19:14.090
back to the digital domain,

00:19:14.090 --> 00:19:15.970
send it to the next processing block,

00:19:15.970 --> 00:19:18.360
and then you convert it into
the analog domain again.

00:19:18.360 --> 00:19:20.490
And that allows you to
preserve the signal.

00:19:20.490 --> 00:19:22.640
- You know, when Rosenblatt
was first setting

00:19:22.640 --> 00:19:23.750
up his perceptron,

00:19:23.750 --> 00:19:26.700
he used a digital IBM computer.

00:19:26.700 --> 00:19:28.330
Finding it too slow,

00:19:28.330 --> 00:19:30.840
he built a custom analog computer,

00:19:30.840 --> 00:19:32.570
complete with variable resistors

00:19:32.570 --> 00:19:35.290
and little motors to drive them.

00:19:35.290 --> 00:19:37.810
Ultimately, his idea of neural networks

00:19:37.810 --> 00:19:39.400
turned out to be right.

00:19:39.400 --> 00:19:42.373
Maybe he was right about analog, too.

00:19:43.250 --> 00:19:46.170
Now, I can't say whether
analog computers will take

00:19:46.170 --> 00:19:48.690
off the way digital did last century,

00:19:48.690 --> 00:19:51.360
but they do seem to be better suited

00:19:51.360 --> 00:19:53.550
to a lot of the tasks
that we want computers

00:19:53.550 --> 00:19:55.210
to perform today,

00:19:55.210 --> 00:19:56.240
which is a little bit funny

00:19:56.240 --> 00:19:58.080
because I always thought of digital

00:19:58.080 --> 00:20:01.530
as the optimal way of
processing information.

00:20:01.530 --> 00:20:03.770
Everything from music to pictures,

00:20:03.770 --> 00:20:07.860
to video has all gone
digital in the last 50 years.

00:20:07.860 --> 00:20:09.670
But maybe in a 100 years,

00:20:09.670 --> 00:20:11.107
we will look back on digital,

00:20:11.107 --> 00:20:15.060
not not as the end point
of information technology,

00:20:15.060 --> 00:20:17.320
but as a starting point.

00:20:17.320 --> 00:20:19.070
Our brains are digital

00:20:19.070 --> 00:20:21.930
in that a neuron either
fires or it doesn't,

00:20:21.930 --> 00:20:24.040
but they're also analog

00:20:24.040 --> 00:20:28.220
in that thinking takes place
everywhere, all at once.

00:20:28.220 --> 00:20:30.070
So maybe what we need

00:20:30.070 --> 00:20:32.490
to achieve true artificial intelligence,

00:20:32.490 --> 00:20:37.134
machines that think like
us, is the power of analog.

00:20:37.134 --> 00:20:39.717
(gentle music)

00:20:42.040 --> 00:20:44.310
Hey, I learned a lot
while making this video,

00:20:44.310 --> 00:20:47.280
much of it by playing with
an actual analog computer.

00:20:47.280 --> 00:20:48.810
You know, trying things out for yourself

00:20:48.810 --> 00:20:50.300
is really the best way to learn,

00:20:50.300 --> 00:20:53.470
and you can do that with this
video sponsor, Brilliant.

00:20:53.470 --> 00:20:54.830
Brilliant is a website and app

00:20:54.830 --> 00:20:56.060
that gets you thinking deeply

00:20:56.060 --> 00:20:58.200
by engaging you in problem-solving.

00:20:58.200 --> 00:21:00.050
They have a great course
on neural networks,

00:21:00.050 --> 00:21:02.460
where you can test how
it works for yourself.

00:21:02.460 --> 00:21:04.290
It gives you an excellent intuition

00:21:04.290 --> 00:21:07.390
about how neural networks can
recognize numbers and shapes,

00:21:07.390 --> 00:21:09.550
and it also allows you to
experience the importance

00:21:09.550 --> 00:21:11.850
of good training data and hidden layers

00:21:11.850 --> 00:21:14.040
to understand why more sophisticated

00:21:14.040 --> 00:21:15.860
neural networks work better.

00:21:15.860 --> 00:21:16.940
What I love about Brilliant

00:21:16.940 --> 00:21:19.320
is it tests your knowledge as you go.

00:21:19.320 --> 00:21:20.810
The lessons are highly interactive,

00:21:20.810 --> 00:21:23.460
and they get progressively
harder as you go on.

00:21:23.460 --> 00:21:26.620
And if you get stuck, there
are always helpful hints.

00:21:26.620 --> 00:21:27.770
For viewers of this video,

00:21:27.770 --> 00:21:29.570
Brilliant is offering the first 200 people

00:21:29.570 --> 00:21:32.100
20% off an annual premium subscription.

00:21:32.100 --> 00:21:35.210
Just go to brilliant.org/veritasium.

00:21:35.210 --> 00:21:37.430
I will put that link
down in the description.

00:21:37.430 --> 00:21:40.030
So I wanna thank Brilliant
for supporting Veritasium,

00:21:40.030 --> 00:21:41.780
and I wanna thank you for watching.

