Start Time,End Time,Text,sentiment-score
00:00:00.000,00:00:01.350,"- For hundreds of years,",0.0
00:00:01.350,00:00:05.100,"analog computers were the most powerful computers on Earth,",0.4754
00:00:05.100,00:00:09.720,"predicting eclipses, tides, and guiding anti-aircraft guns.",0.0
00:00:09.720,00:00:12.650,"Then, with the advent of solid-state transistors,",0.0
00:00:12.650,00:00:14.500,digital computers took off.,0.0
00:00:14.500,00:00:18.080,"Now, virtually every computer we use is digital.",0.0
00:00:18.080,00:00:21.730,"But today, a perfect storm of factors is setting the scene",0.7227
00:00:21.730,00:00:24.900,for a resurgence of analog technology.,0.0
00:00:24.900,00:00:27.530,"This is an analog computer,",0.0
00:00:27.530,00:00:30.430,"and by connecting these wires in particular ways,",0.0
00:00:30.430,00:00:32.670,I can program it to solve a whole range,0.2023
00:00:32.670,00:00:34.870,of differential equations.,0.0
00:00:34.870,00:00:37.740,"For example, this setup allows me to simulate",0.0
00:00:37.740,00:00:40.800,a damped mass oscillating on a spring.,0.0
00:00:40.800,00:00:43.720,"So on the oscilloscope, you can actually see the position",0.0
00:00:43.720,00:00:45.750,of the mass over time.,0.0
00:00:45.750,00:00:48.940,"And I can vary the damping,",0.0
00:00:48.940,00:00:51.900,"or the spring constant,",0.0
00:00:51.900,00:00:54.740,"or the mass, and we can see how the amplitude",0.0
00:00:54.740,00:00:57.740,and duration of the oscillations change.,0.0
00:00:57.740,00:01:00.060,Now what makes this an analog computer,0.0
00:01:00.060,00:01:03.440,is that there are no zeros and ones in here.,-0.296
00:01:03.440,00:01:06.810,"Instead, there's actually a voltage that oscillates",0.0
00:01:06.810,00:01:10.260,up and down exactly like a mass on a spring.,0.3612
00:01:10.260,00:01:12.770,The electrical circuitry is an analog,0.0
00:01:12.770,00:01:14.330,"for the physical problem,",-0.4019
00:01:14.330,00:01:16.660,it just takes place much faster.,0.0
00:01:16.660,00:01:19.120,"Now, if I change the electrical connections,",0.0
00:01:19.120,00:01:20.300,I can program this computer,0.0
00:01:20.300,00:01:22.270,"to solve other differential equations,",0.2023
00:01:22.270,00:01:24.010,"like the Lorenz system,",0.3612
00:01:24.010,00:01:27.290,which is a basic model of convection in the atmosphere.,0.0
00:01:27.290,00:01:29.460,Now the Lorenz system is famous because it was one,0.0
00:01:29.460,00:01:32.270,of the first discovered examples of chaos.,-0.5719
00:01:32.270,00:01:35.520,"And here, you can see the Lorenz attractor",0.296
00:01:35.520,00:01:38.400,with its beautiful butterfly shape.,0.5994
00:01:38.400,00:01:39.900,"And on this analog computer,",0.0
00:01:39.900,00:01:42.360,I can change the parameters,0.0
00:01:42.360,00:01:45.563,and see their effects in real time.,0.0
00:01:46.410,00:01:47.980,So these examples illustrate some,0.0
00:01:47.980,00:01:50.650,of the advantages of analog computers.,0.3612
00:01:50.650,00:01:53.260,"They are incredibly powerful computing devices,",0.4754
00:01:53.260,00:01:56.580,and they can complete a lot of computations fast.,0.0
00:01:56.580,00:01:59.133,"Plus, they don't take much power to do it.",0.0
00:02:01.520,00:02:02.900,"With a digital computer,",0.0
00:02:02.900,00:02:05.600,"if you wanna add two eight-bit numbers,",0.0
00:02:05.600,00:02:08.120,"you need around 50 transistors,",0.0
00:02:08.120,00:02:09.760,"whereas with an analog computer,",0.0
00:02:09.760,00:02:12.240,"you can add two currents,",0.0
00:02:12.240,00:02:15.760,just by connecting two wires.,0.0
00:02:15.760,00:02:18.380,"With a digital computer to multiply two numbers,",0.0
00:02:18.380,00:02:20.970,"you need on the order of 1,000 transistors",0.0
00:02:20.970,00:02:23.600,"all switching zeros and ones,",0.0
00:02:23.600,00:02:24.960,"whereas with an analog computer,",0.0
00:02:24.960,00:02:28.480,"you can pass a current through a resistor,",0.0
00:02:28.480,00:02:31.850,and then the voltage across this resistor,0.0
00:02:31.850,00:02:34.290,will be I times R.,0.0
00:02:34.290,00:02:35.700,"So effectively,",0.4927
00:02:35.700,00:02:38.883,you have multiplied two numbers together.,0.0
00:02:40.010,00:02:42.930,But analog computers also have their drawbacks.,0.0
00:02:42.930,00:02:43.763,"For one thing,",0.0
00:02:43.763,00:02:46.430,they are not general-purpose computing devices.,0.0
00:02:46.430,00:02:49.430,"I mean, you're not gonna run Microsoft Word on this thing.",0.0
00:02:49.430,00:02:52.990,"And also, since the inputs and outputs are continuous,",0.0
00:02:52.990,00:02:55.920,I can't input exact values.,-0.3089
00:02:55.920,00:02:58.980,"So if I try to repeat the same calculation,",0.0
00:02:58.980,00:03:01.870,I'm never going to get the exact same answer.,0.0
00:03:01.870,00:03:04.660,"Plus, think about manufacturing analog computers.",0.0
00:03:04.660,00:03:06.300,There's always gonna be some variation,0.0
00:03:06.300,00:03:08.200,"in the exact value of components,",0.34
00:03:08.200,00:03:10.370,like resistors or capacitors.,0.3612
00:03:10.370,00:03:12.260,"So as a general rule of thumb,",0.0
00:03:12.260,00:03:15.540,you can expect about a 1% error.,-0.4019
00:03:15.540,00:03:17.340,"So when you think of analog computers,",0.0
00:03:17.340,00:03:20.820,"you can think powerful, fast, and energy-efficient,",0.4215
00:03:20.820,00:03:25.730,"but also single-purpose, non-repeatable, and inexact.",0.0
00:03:25.730,00:03:28.140,"And if those sound like deal-breakers,",0.3612
00:03:28.140,00:03:30.010,it's because they probably are.,0.0
00:03:30.010,00:03:31.850,I think these are the major reasons,0.0
00:03:31.850,00:03:33.610,why analog computers fell out of favor,0.4019
00:03:33.610,00:03:36.900,as soon as digital computers became viable.,0.0
00:03:36.900,00:03:41.318,"Now, here's why analog computers may be making a comeback.",0.0
00:03:41.318,00:03:43.800,(computers beeping),0.0
00:03:43.800,00:03:46.500,It all starts with artificial intelligence.,0.4767
00:03:46.500,00:03:48.710,- [Narrator] A machine has been programmed to see,0.0
00:03:48.710,00:03:50.043,and to move objects.,0.0
00:03:51.330,00:03:52.940,- AI isn't new.,0.0
00:03:52.940,00:03:55.690,The term was coined back in 1956.,0.0
00:03:55.690,00:03:58.760,"In 1958, Cornell University psychologist,",0.0
00:03:58.760,00:04:01.290,"Frank Rosenblatt, built the perceptron,",0.0
00:04:01.290,00:04:05.150,designed to mimic how neurons fire in our brains.,-0.34
00:04:05.150,00:04:08.930,So here's a basic model of how neurons in our brains work.,0.0
00:04:08.930,00:04:12.160,"An individual neuron can either fire or not,",-0.34
00:04:12.160,00:04:14.380,so its level of activation can be represented,0.0
00:04:14.380,00:04:16.480,as a one or a zero.,0.0
00:04:16.480,00:04:18.370,The input to one neuron,0.0
00:04:18.370,00:04:21.130,"is the output from a bunch other neurons,",0.0
00:04:21.130,00:04:22.820,but the strength of these connections,0.6486
00:04:22.820,00:04:24.410,"between neurons varies,",0.0
00:04:24.410,00:04:27.420,so each one can be given a different weight.,0.0
00:04:27.420,00:04:29.410,"Some connections are excitatory,",0.2732
00:04:29.410,00:04:30.910,"so they have positive weights,",0.5945
00:04:30.910,00:04:32.840,"while others are inhibitory,",-0.25
00:04:32.840,00:04:34.550,so they have negative weights.,-0.6077
00:04:34.550,00:04:35.470,And the way to figure out,0.0
00:04:35.470,00:04:37.670,"whether a particular neuron fires,",0.0
00:04:37.670,00:04:40.380,is to take the activation of each input neuron,0.0
00:04:40.380,00:04:42.600,"and multiply by its weight,",0.0
00:04:42.600,00:04:44.370,and then add these all together.,0.0
00:04:44.370,00:04:47.490,"If their sum is greater than some number called the bias,",0.34
00:04:47.490,00:04:49.050,"then the neuron fires,",0.0
00:04:49.050,00:04:51.963,"but if it's less than that, the neuron doesn't fire.",0.3724
00:04:53.460,00:04:57.190,"As input, Rosenblatt's perceptron had 400 photocells",0.0
00:04:57.190,00:04:59.080,"arranged in a square grid,",0.0
00:04:59.080,00:05:02.160,to capture a 20 by 20-pixel image.,0.0
00:05:02.160,00:05:04.670,"You can think of each pixel as an input neuron,",0.0
00:05:04.670,00:05:07.780,with its activation being the brightness of the pixel.,0.3818
00:05:07.780,00:05:09.020,"Although strictly speaking,",0.0
00:05:09.020,00:05:11.910,"the activation should be either zero or one,",0.0
00:05:11.910,00:05:15.960,we can let it take any value between zero and one.,0.34
00:05:15.960,00:05:18.090,All of these neurons are connected,0.0
00:05:18.090,00:05:20.130,"to a single output neuron,",0.0
00:05:20.130,00:05:23.140,each via its own adjustable weight.,0.0
00:05:23.140,00:05:25.370,"So to see if the output neuron will fire,",-0.34
00:05:25.370,00:05:28.840,"you multiply the activation of each neuron by its weight,",0.0
00:05:28.840,00:05:30.440,and add them together.,0.0
00:05:30.440,00:05:33.260,This is essentially a vector dot product.,0.0
00:05:33.260,00:05:36.600,"If the answer is larger than the bias, the neuron fires,",-0.1027
00:05:36.600,00:05:38.890,"and if not, it doesn't.",0.0
00:05:38.890,00:05:40.590,Now the goal of the perceptron,0.0
00:05:40.590,00:05:43.690,"was to reliably distinguish between two images,",0.0
00:05:43.690,00:05:45.970,like a rectangle and a circle.,0.3612
00:05:45.970,00:05:46.803,"For example,",0.0
00:05:46.803,00:05:48.600,the output neuron could always fire,-0.34
00:05:48.600,00:05:49.950,"when presented with a circle,",0.0
00:05:49.950,00:05:52.930,but never when presented with a rectangle.,0.0
00:05:52.930,00:05:55.890,"To achieve this, the perception had to be trained,",0.0
00:05:55.890,00:05:58.410,"that is, shown a series of different circles",0.0
00:05:58.410,00:06:02.490,"and rectangles, and have its weights adjusted accordingly.",0.0
00:06:02.490,00:06:05.350,"We can visualize the weights as an image,",0.0
00:06:05.350,00:06:09.440,since there's a unique weight for each pixel of the image.,0.0
00:06:09.440,00:06:12.470,"Initially, Rosenblatt set all the weights to zero.",0.0
00:06:12.470,00:06:14.530,"If the perceptron's output is correct,",0.0
00:06:14.530,00:06:16.820,"for example, here it's shown a rectangle",0.0
00:06:16.820,00:06:19.000,"and the output neuron doesn't fire,",0.2584
00:06:19.000,00:06:21.260,no change is made to the weights.,-0.296
00:06:21.260,00:06:23.910,"But if it's wrong, then the weights are adjusted.",-0.631
00:06:23.910,00:06:25.920,The algorithm for updating the weights,0.0
00:06:25.920,00:06:27.640,is remarkably simple.,0.0
00:06:27.640,00:06:30.730,"Here, the output neuron didn't fire when it was supposed to",0.2584
00:06:30.730,00:06:32.370,because it was shown a circle.,0.0
00:06:32.370,00:06:33.890,"So to modify the weights,",0.0
00:06:33.890,00:06:38.210,you simply add the input activations to the weights.,0.0
00:06:38.210,00:06:40.540,"If the output neuron fires when it shouldn't,",0.0
00:06:40.540,00:06:42.890,"like here, when shown a rectangle,",0.3612
00:06:42.890,00:06:45.670,"well, then you subtract the input activations",0.2732
00:06:45.670,00:06:48.280,"from the weights, and you keep doing this",0.0
00:06:48.280,00:06:50.840,until the perceptron correctly identifies,0.0
00:06:50.840,00:06:52.630,all the training images.,0.0
00:06:52.630,00:06:55.520,"It was shown that this algorithm will always converge,",0.0
00:06:55.520,00:06:58.180,so long as it's possible to map the two categories,0.0
00:06:58.180,00:07:00.035,into distinct groups.,0.0
00:07:00.035,00:07:02.240,(footsteps thumping),0.0
00:07:02.240,00:07:04.960,The perceptron was capable of distinguishing,0.3818
00:07:04.960,00:07:07.900,"between different shapes, like rectangles and triangles,",0.3612
00:07:07.900,00:07:09.220,or between different letters.,0.0
00:07:09.220,00:07:10.650,"And according to Rosenblatt,",0.0
00:07:10.650,00:07:14.020,it could even tell the difference between cats and dogs.,0.0
00:07:14.020,00:07:15.910,He said the machine was capable,0.3818
00:07:15.910,00:07:18.940,"of what amounts to original thought,",0.3182
00:07:18.940,00:07:20.880,and the media lapped it up.,0.0
00:07:20.880,00:07:22.887,"The ""New York Times"" called the perceptron",0.0
00:07:22.887,00:07:25.280,"""the embryo of an electronic computer",0.0
00:07:25.280,00:07:28.350,"that the Navy expects will be able to walk, talk,",0.0
00:07:28.350,00:07:30.890,"see, write, reproduce itself,",0.0
00:07:30.890,00:07:33.647,"and be conscious of its existence.""",0.0
00:07:34.750,00:07:36.950,"- [Narrator] After training on lots of examples,",0.0
00:07:36.950,00:07:39.533,"it's given new faces it has never seen,",0.0
00:07:39.533,00:07:43.480,and is able to successfully distinguish male from female.,0.4939
00:07:43.480,00:07:45.020,It has learned.,0.0
00:07:45.020,00:07:47.530,"- In reality, the perceptron was pretty limited",0.3182
00:07:47.530,00:07:48.730,in what it could do.,0.0
00:07:48.730,00:07:52.050,"It could not, in fact, tell apart dogs from cats.",0.0
00:07:52.050,00:07:53.950,This and other critiques were raised,0.0
00:07:53.950,00:07:58.187,"in a book by MIT giants, Minsky and Papert, in 1969.",0.0
00:07:58.187,00:08:00.330,And that led to a bust period,0.0
00:08:00.330,00:08:03.530,for artificial neural networks and AI in general.,0.0
00:08:03.530,00:08:06.720,It's known as the first AI winter.,0.0
00:08:06.720,00:08:09.370,Rosenblatt did not survive this winter.,0.0
00:08:09.370,00:08:12.200,He drowned while sailing in Chesapeake Bay,-0.5994
00:08:12.200,00:08:14.118,on his 43rd birthday.,0.0
00:08:14.118,00:08:17.330,(mellow upbeat music),0.0
00:08:17.330,00:08:19.660,"- [Narrator] The NAV Lab is a road-worthy truck,",0.0
00:08:19.660,00:08:22.290,modified so that researchers or computers,0.0
00:08:22.290,00:08:25.360,can control the vehicle as occasion demands.,0.0
00:08:25.360,00:08:28.090,"- [Derek] In the 1980s, there was an AI resurgence",0.0
00:08:28.090,00:08:30.090,when researchers at Carnegie Mellon created one,0.25
00:08:30.090,00:08:32.470,of the first self-driving cars.,0.0
00:08:32.470,00:08:33.860,The vehicle was steered,0.0
00:08:33.860,00:08:36.900,by an artificial neural network called ALVINN.,0.0
00:08:36.900,00:08:37.950,"It was similar to the perceptron,",0.0
00:08:37.950,00:08:41.080,except it had a hidden layer of artificial neurons,0.0
00:08:41.080,00:08:43.300,between the input and output.,0.0
00:08:43.300,00:08:47.060,"As input, ALVINN received 30 by 32-pixel images",0.0
00:08:47.060,00:08:48.400,of the road ahead.,0.0
00:08:48.400,00:08:51.600,"Here, I'm showing them as 60 by 64 pixels.",0.0
00:08:51.600,00:08:54.020,But each of these input neurons was connected,0.0
00:08:54.020,00:08:57.950,via an adjustable weight to a hidden layer of four neurons.,0.0
00:08:57.950,00:09:01.600,These were each connected to 32 output neurons.,0.0
00:09:01.600,00:09:04.180,"So to go from one layer of the network to the next,",0.0
00:09:04.180,00:09:06.860,you perform a matrix multiplication:,0.0
00:09:06.860,00:09:10.120,the input activation times the weights.,0.0
00:09:10.120,00:09:12.580,The output neuron with the greatest activation,0.6369
00:09:12.580,00:09:14.483,determines the steering angle.,0.0
00:09:15.540,00:09:16.920,"To train the neural net,",0.0
00:09:16.920,00:09:18.550,"a human drove the vehicle,",0.0
00:09:18.550,00:09:20.900,providing the correct steering angle,0.0
00:09:20.900,00:09:22.720,for a given input image.,0.0
00:09:22.720,00:09:24.810,All the weights in the neural network were adjusted,0.0
00:09:24.810,00:09:25.643,through the training,0.0
00:09:25.643,00:09:27.850,so that ALVINN's output better matched that,0.4404
00:09:27.850,00:09:29.063,of the human driver.,0.0
00:09:30.270,00:09:31.780,The method for adjusting the weights,0.0
00:09:31.780,00:09:33.390,"is called backpropagation,",0.0
00:09:33.390,00:09:34.930,"which I won't go into here,",0.0
00:09:34.930,00:09:37.350,"but Welch Labs has a great series on this,",0.7684
00:09:37.350,00:09:39.253,which I'll link to in the description.,0.0
00:09:40.090,00:09:41.900,"Again, you can visualize the weights",0.0
00:09:41.900,00:09:44.500,for the four hidden neurons as images.,0.0
00:09:44.500,00:09:46.720,"The weights are initially set to be random,",0.0
00:09:46.720,00:09:48.210,"but as training progresses,",0.0
00:09:48.210,00:09:51.760,the computer learns to pick up on certain patterns.,0.2732
00:09:51.760,00:09:54.890,You can see the road markings emerge in the weights.,0.0
00:09:54.890,00:09:58.190,"Simultaneously, the output steering angle coalesces",0.0
00:09:58.190,00:10:00.620,onto the human steering angle.,0.0
00:10:00.620,00:10:03.080,The computer drove the vehicle at a top speed,0.2023
00:10:03.080,00:10:06.350,of around one or two kilometers per hour.,0.0
00:10:06.350,00:10:07.830,It was limited by the speed,-0.2263
00:10:07.830,00:10:10.763,at which the computer could perform matrix multiplication.,0.0
00:10:12.250,00:10:13.700,"Despite these advances,",0.0
00:10:13.700,00:10:15.810,artificial neural networks still struggled,-0.34
00:10:15.810,00:10:17.550,"with seemingly simple tasks,",0.0
00:10:17.550,00:10:19.920,like telling apart cats and dogs.,0.3612
00:10:19.920,00:10:22.250,And no one knew whether hardware,-0.296
00:10:22.250,00:10:24.210,or software was the weak link.,-0.4404
00:10:24.210,00:10:26.560,"I mean, did we have a good model of intelligence,",0.7184
00:10:26.560,00:10:28.590,we just needed more computer power?,0.0
00:10:28.590,00:10:30.470,"Or, did we have the wrong idea",-0.4767
00:10:30.470,00:10:33.550,about how to make intelligence systems altogether?,0.4767
00:10:33.550,00:10:36.140,So artificial intelligence experienced another lull,0.5233
00:10:36.140,00:10:38.150,in the 1990s.,0.0
00:10:38.150,00:10:39.440,"By the mid 2000s,",0.0
00:10:39.440,00:10:43.250,most AI researchers were focused on improving algorithms.,0.6597
00:10:43.250,00:10:45.870,"But one researcher, Fei-Fei Li,",0.0
00:10:45.870,00:10:48.260,thought maybe there was a different problem.,-0.4019
00:10:48.260,00:10:50.350,Maybe these artificial neural networks,0.0
00:10:50.350,00:10:52.540,just needed more data to train on.,0.0
00:10:52.540,00:10:56.170,So she planned to map out the entire world of objects.,0.0
00:10:56.170,00:10:59.220,"From 2006 to 2009, she created ImageNet,",0.25
00:10:59.220,00:11:02.550,"a database of 1.2 million human-labeled images,",0.0
00:11:02.550,00:11:03.383,"which at the time,",0.0
00:11:03.383,00:11:06.330,was the largest labeled image dataset ever constructed.,0.0
00:11:06.330,00:11:08.258,"And from 2010 to 2017,",0.0
00:11:08.258,00:11:10.400,ImageNet ran an annual contest:,0.0
00:11:10.400,00:11:13.740,"the ImageNet Large Scale Visual Recognition Challenge,",0.0772
00:11:13.740,00:11:16.380,where software programs competed to correctly detect,0.0
00:11:16.380,00:11:17.970,and classify images.,0.0
00:11:17.970,00:11:21.110,"Images were classified into 1,000 different categories,",0.0
00:11:21.110,00:11:23.520,including 90 different dog breeds.,0.0
00:11:23.520,00:11:25.460,A neural network competing in this competition,0.0
00:11:25.460,00:11:28.220,"would have an output layer of 1,000 neurons,",0.0
00:11:28.220,00:11:30.460,each corresponding to a category of object,0.0
00:11:30.460,00:11:32.230,that could appear in the image.,0.0
00:11:32.230,00:11:34.500,"If the image contains, say, a German shepherd,",0.0
00:11:34.500,00:11:37.430,then the output neuron corresponding to German shepherd,0.0
00:11:37.430,00:11:39.770,should have the highest activation.,0.0
00:11:39.770,00:11:43.120,"Unsurprisingly, it turned out to be a tough challenge.",-0.0516
00:11:43.120,00:11:45.130,One way to judge the performance of an AI,0.0
00:11:45.130,00:11:48.360,is to see how often the five highest neuron activations,0.0
00:11:48.360,00:11:50.920,do not include the correct category.,0.0
00:11:50.920,00:11:53.840,This is the so-called top-5 error rate.,-0.4019
00:11:53.840,00:11:56.920,"In 2010, the best performer had a top-5 error rate",0.3612
00:11:56.920,00:12:01.080,"of 28.2%, meaning that nearly 1/3 of the time,",0.0
00:12:01.080,00:12:04.570,the correct answer was not among its top five guesses.,-0.1511
00:12:04.570,00:12:09.270,"In 2011, the error rate of the best performer was 25.8%,",0.3612
00:12:09.270,00:12:11.360,a substantial improvement.,0.5859
00:12:11.360,00:12:12.370,"But the next year,",0.0
00:12:12.370,00:12:13.620,an artificial neural network,0.0
00:12:13.620,00:12:16.220,"from the University of Toronto, called AlexNet,",0.0
00:12:16.220,00:12:17.870,blew away the competition,0.0
00:12:17.870,00:12:22.410,with a top-5 error rate of just 16.4%.,-0.4019
00:12:22.410,00:12:25.840,What set AlexNet apart was its size and depth.,0.0
00:12:25.840,00:12:27.720,"The network consisted of eight layers,",0.0
00:12:27.720,00:12:30.700,"and in total, 500,000 neurons.",0.0
00:12:30.700,00:12:31.533,"To train AlexNet,",0.0
00:12:31.533,00:12:35.580,60 million weights and biases had to be carefully adjusted,0.128
00:12:35.580,00:12:37.500,using the training database.,0.0
00:12:37.500,00:12:40.040,"Because of all the big matrix multiplications,",0.0
00:12:40.040,00:12:43.570,processing a single image required 700 million,0.0
00:12:43.570,00:12:45.330,individual math operations.,0.0
00:12:45.330,00:12:48.280,So training was computationally intensive.,0.0
00:12:48.280,00:12:51.330,"The team managed it by pioneering the use of GPUs,",0.0
00:12:51.330,00:12:52.910,"graphical processing units,",0.0
00:12:52.910,00:12:56.330,"which are traditionally used for driving displays, screens.",0.0
00:12:56.330,00:13:00.110,So they're specialized for fast parallel computations.,0.0
00:13:00.110,00:13:02.520,The AlexNet paper describing their research,0.0
00:13:02.520,00:13:04.260,is a blockbuster.,0.5994
00:13:04.260,00:13:07.680,"It's now been cited over 100,000 times,",0.0
00:13:07.680,00:13:10.390,and it identifies the scale of the neural network,0.0
00:13:10.390,00:13:12.980,as key to its success.,0.5719
00:13:12.980,00:13:16.160,"It takes a lot of computation to train and run the network,",0.0
00:13:16.160,00:13:19.290,but the improvement in performance is worth it.,0.7469
00:13:19.290,00:13:20.740,"With others following their lead,",0.0
00:13:20.740,00:13:22.010,the top-5 error rate,-0.4019
00:13:22.010,00:13:23.900,on the ImageNet competition plummeted,0.0
00:13:23.900,00:13:28.210,"in the years that followed, down to 3.6% in 2015.",0.0
00:13:28.210,00:13:31.160,That is better than human performance.,0.4404
00:13:31.160,00:13:32.730,The neural network that achieved this,0.0
00:13:32.730,00:13:35.030,had 100 layers of neurons.,0.0
00:13:35.030,00:13:36.490,So the future is clear:,0.3818
00:13:36.490,00:13:38.390,We will see ever increasing demand,-0.128
00:13:38.390,00:13:40.820,for ever larger neural networks.,0.0
00:13:40.820,00:13:43.040,And this is a problem for several reasons:,-0.4019
00:13:43.040,00:13:45.150,One is energy consumption.,0.2732
00:13:45.150,00:13:47.060,Training a neural network requires an amount,0.0
00:13:47.060,00:13:49.460,of electricity similar to the yearly consumption,0.0
00:13:49.460,00:13:50.990,of three households.,0.0
00:13:50.990,00:13:54.100,Another issue is the so-called Von Neumann Bottleneck.,0.0
00:13:54.100,00:13:55.870,Virtually every modern digital computer,0.0
00:13:55.870,00:13:57.200,"stores data in memory,",0.0
00:13:57.200,00:14:00.360,and then accesses it as needed over a bus.,0.0
00:14:00.360,00:14:02.840,When performing the huge matrix multiplications required,0.3182
00:14:02.840,00:14:04.230,"by deep neural networks,",0.0
00:14:04.230,00:14:05.670,most of the time and energy goes,0.2732
00:14:05.670,00:14:07.960,into fetching those weight values rather,0.4019
00:14:07.960,00:14:10.490,than actually doing the computation.,0.0
00:14:10.490,00:14:13.250,"And finally, there are the limitations of Moore's Law.",0.0
00:14:13.250,00:14:14.960,"For decades, the number of transistors",0.0772
00:14:14.960,00:14:18.190,"on a chip has been doubling approximately every two years,",0.0
00:14:18.190,00:14:20.130,but now the size of a transistor,0.0
00:14:20.130,00:14:21.910,is approaching the size of an atom.,0.0
00:14:21.910,00:14:24.690,So there are some fundamental physical challenges,0.0772
00:14:24.690,00:14:26.880,to further miniaturization.,0.0
00:14:26.880,00:14:30.270,So this is the perfect storm for analog computers.,0.5719
00:14:30.270,00:14:32.640,Digital computers are reaching their limits.,0.2023
00:14:32.640,00:14:35.880,"Meanwhile, neural networks are exploding in popularity,",0.4767
00:14:35.880,00:14:38.000,and a lot of what they do boils down,0.0
00:14:38.000,00:14:41.350,to a single task: matrix multiplication.,0.0
00:14:41.350,00:14:44.070,"Best of all, neural networks don't need the precision",0.6369
00:14:44.070,00:14:45.350,of digital computers.,0.0
00:14:45.350,00:14:48.990,Whether the neural net is 96% or 98% confident,0.4939
00:14:48.990,00:14:50.410,"the image contains a chicken,",0.0
00:14:50.410,00:14:52.810,"it doesn't really matter, it's still a chicken.",-0.0749
00:14:52.810,00:14:54.870,So slight variability in components,0.0
00:14:54.870,00:14:57.372,or conditions can be tolerated.,0.0
00:14:57.372,00:14:58.810,(upbeat rock music),0.0
00:14:58.810,00:15:01.370,"I went to an analog computing startup in Texas,",0.0
00:15:01.370,00:15:03.370,called Mythic AI.,0.0
00:15:03.370,00:15:06.820,"Here, they're creating analog chips to run neural networks.",0.296
00:15:06.820,00:15:09.923,And they demonstrated several AI algorithms for me.,0.0
00:15:10.980,00:15:11.813,"- Oh, there you go.",0.0
00:15:11.813,00:15:13.290,"See, it's getting you. (Derek laughs)",0.4939
00:15:13.290,00:15:14.770,Yeah. - That's fascinating.,0.6908
00:15:14.770,00:15:17.630,- The biggest use case is augmented in virtual reality.,0.0
00:15:17.630,00:15:19.150,"If your friend is in a different,",0.4939
00:15:19.150,00:15:20.710,"they're at their house and you're at your house,",0.0
00:15:20.710,00:15:24.120,you can actually render each other in the virtual world.,0.0
00:15:24.120,00:15:27.280,"So it needs to really quickly capture your pose,",0.0
00:15:27.280,00:15:29.350,and then render it in the VR world.,0.0
00:15:29.350,00:15:31.333,"- So, hang on, is this for the metaverse thing?",0.0
00:15:31.333,00:15:35.640,"- Yeah, this is a very metaverse application.",0.296
00:15:35.640,00:15:38.630,This is depth estimation from just a single webcam.,0.0
00:15:38.630,00:15:39.950,"It's just taking this scene,",0.0
00:15:39.950,00:15:41.360,and then it's doing a heat map.,0.0
00:15:41.360,00:15:43.750,"So if it's bright, it means it's close.",0.4877
00:15:43.750,00:15:45.980,"And if it's far away, it makes it black.",0.0
00:15:45.980,00:15:47.560,- [Derek] Now all these algorithms can be run,0.0
00:15:47.560,00:15:48.850,"on digital computers,",0.0
00:15:48.850,00:15:52.250,"but here, the matrix multiplication is actually taking place",0.0
00:15:52.250,00:15:54.420,in the analog domain. (light music),0.0
00:15:54.420,00:15:55.800,"To make this possible,",0.0
00:15:55.800,00:15:59.490,Mythic has repurposed digital flash storage cells.,0.0
00:15:59.490,00:16:01.210,Normally these are used as memory,0.0
00:16:01.210,00:16:03.630,to store either a one or a zero.,0.0
00:16:03.630,00:16:07.460,"If you apply a large positive voltage to the control gate,",0.5574
00:16:07.460,00:16:10.210,electrons tunnel up through an insulating barrier,-0.128
00:16:10.210,00:16:12.440,and become trapped on the floating gate.,-0.5267
00:16:12.440,00:16:13.570,"Remove the voltage,",0.0
00:16:13.570,00:16:15.440,and the electrons can remain on the floating gate,0.0
00:16:15.440,00:16:18.609,"for decades, preventing the cell from conducting current.",-0.0258
00:16:18.609,00:16:21.320,And that's how you can store either a one or a zero.,0.0
00:16:21.320,00:16:22.960,You can read out the stored value,0.34
00:16:22.960,00:16:25.060,by applying a small voltage.,0.0
00:16:25.060,00:16:26.970,"If there are electrons on the floating gate,",0.0
00:16:26.970,00:16:29.580,"no current flows, so that's a zero.",-0.296
00:16:29.580,00:16:30.940,"If there aren't electrons,",0.0
00:16:30.940,00:16:33.920,"then current does flow, and that's a one.",0.0
00:16:33.920,00:16:36.030,Now Mythic's idea is to use these cells,0.0
00:16:36.030,00:16:40.000,"not as on/off switches, but as variable resistors.",0.0
00:16:40.000,00:16:42.900,They do this by putting a specific number of electrons,0.0772
00:16:42.900,00:16:45.670,"on each floating gate, instead of all or nothing.",0.0
00:16:45.670,00:16:47.330,"The greater the number of electrons,",0.4215
00:16:47.330,00:16:49.810,the higher the resistance of the channel.,0.0
00:16:49.810,00:16:52.000,"When you later apply a small voltage,",0.0
00:16:52.000,00:16:55.720,the current that flows is equal to V over R.,0.0
00:16:55.720,00:16:59.250,"But you can also think of this as voltage times conductance,",0.0
00:16:59.250,00:17:02.510,where conductance is just the reciprocal of resistance.,0.0
00:17:02.510,00:17:04.470,So a single flash cell can be used,0.0
00:17:04.470,00:17:09.130,"to multiply two values together, voltage times conductance.",0.4019
00:17:09.130,00:17:11.860,"So to use this to run an artificial neural network,",0.0
00:17:11.860,00:17:14.750,well they first write all the weights to the flash cells,0.2732
00:17:14.750,00:17:16.870,as each cell's conductance.,0.0
00:17:16.870,00:17:19.270,"Then, they input the activation values",0.4019
00:17:19.270,00:17:21.490,as the voltage on the cells.,0.0
00:17:21.490,00:17:23.630,And the resulting current is the product,0.0
00:17:23.630,00:17:25.500,"of voltage times conductance,",0.0
00:17:25.500,00:17:28.300,which is activation times weight.,0.0
00:17:28.300,00:17:30.860,The cells are wired together in such a way,0.0
00:17:30.860,00:17:34.000,"that the current from each multiplication adds together,",0.0
00:17:34.000,00:17:36.607,completing the matrix multiplication.,0.0
00:17:36.607,00:17:39.040,(light music),0.0
00:17:39.040,00:17:40.920,- So this is our first product.,0.0
00:17:40.920,00:17:45.860,This can do 25 trillion math operations per second.,0.0
00:17:45.860,00:17:47.040,- [Derek] 25 trillion.,0.0
00:17:47.040,00:17:49.400,"- Yep, 25 trillion math operations per second,",0.296
00:17:49.400,00:17:50.540,"in this little chip here,",0.0
00:17:50.540,00:17:52.620,burning about three watts of power.,0.0
00:17:52.620,00:17:54.930,- [Derek] How does it compare to a digital chip?,0.0
00:17:54.930,00:17:57.900,- The newer digital systems can do anywhere,0.0
00:17:57.900,00:18:00.680,"from 25 to 100 trillion operations per second,",0.0
00:18:00.680,00:18:02.780,"but they are big, thousand-dollar systems",0.0
00:18:02.780,00:18:06.590,that are spitting out 50 to 100 watts of power.,0.0
00:18:06.590,00:18:07.680,- [Derek] Obviously this isn't,0.0
00:18:07.680,00:18:09.460,"like an apples apples comparison, right?",0.3612
00:18:09.460,00:18:10.580,"- No, it's not apples to apples.",0.0
00:18:10.580,00:18:13.660,"I mean, training those algorithms,",0.0
00:18:13.660,00:18:15.510,you need big hardware like this.,0.3612
00:18:15.510,00:18:17.610,"You can just do all sorts of stuff on the GPU,",0.0
00:18:17.610,00:18:20.360,but if you specifically are doing AI workloads,0.0
00:18:20.360,00:18:22.720,"and you wanna deploy 'em, you could use this instead.",0.0
00:18:22.720,00:18:25.170,"You can imagine them in security cameras,",0.34
00:18:25.170,00:18:26.690,"autonomous systems,",0.0
00:18:26.690,00:18:29.120,inspection equipment for manufacturing.,0.0
00:18:29.120,00:18:30.880,"Every time they make a Frito-Lay chip,",0.0
00:18:30.880,00:18:32.200,"they inspect it with a camera,",0.0
00:18:32.200,00:18:36.170,and the bad Fritos get blown off of the conveyor belt.,-0.5423
00:18:36.170,00:18:37.750,But they're using artificial intelligence,0.631
00:18:37.750,00:18:40.410,to spot which Fritos are good and bad.,-0.1531
00:18:40.410,00:18:42.650,- Some have proposed using analog circuitry,0.0
00:18:42.650,00:18:43.920,"in smart home speakers,",0.4019
00:18:43.920,00:18:47.650,"solely to listen for the wake word, like Alexa or Siri.",0.3612
00:18:47.650,00:18:49.940,They would use a lot less power and be able to quickly,0.0
00:18:49.940,00:18:53.310,and reliably turn on the digital circuitry of the device.,0.0
00:18:53.310,00:18:56.420,But you still have to deal with the challenges of analog.,0.1154
00:18:56.420,00:18:58.120,"- So for one of the popular networks,",0.4215
00:18:58.120,00:19:00.740,there would be 50 sequences,0.0
00:19:00.740,00:19:02.670,of matrix multiplies that you're doing.,0.0
00:19:02.670,00:19:05.040,"Now, if you did that entirely in the analog domain,",0.0
00:19:05.040,00:19:06.340,"by the time it gets to the output,",0.0
00:19:06.340,00:19:07.810,it's just so distorted,-0.541
00:19:07.810,00:19:10.260,that you don't have any result at all.,0.0
00:19:10.260,00:19:12.130,"So you convert it from the analog domain,",0.0
00:19:12.130,00:19:14.090,"back to the digital domain,",0.0
00:19:14.090,00:19:15.970,"send it to the next processing block,",-0.4404
00:19:15.970,00:19:18.360,and then you convert it into the analog domain again.,0.0
00:19:18.360,00:19:20.490,And that allows you to preserve the signal.,0.0
00:19:20.490,00:19:22.640,"- You know, when Rosenblatt was first setting",0.0
00:19:22.640,00:19:23.750,"up his perceptron,",0.0
00:19:23.750,00:19:26.700,he used a digital IBM computer.,0.0
00:19:26.700,00:19:28.330,"Finding it too slow,",0.0
00:19:28.330,00:19:30.840,"he built a custom analog computer,",0.0
00:19:30.840,00:19:32.570,complete with variable resistors,0.0
00:19:32.570,00:19:35.290,and little motors to drive them.,0.0
00:19:35.290,00:19:37.810,"Ultimately, his idea of neural networks",0.0
00:19:37.810,00:19:39.400,turned out to be right.,0.0
00:19:39.400,00:19:42.373,"Maybe he was right about analog, too.",0.0
00:19:43.250,00:19:46.170,"Now, I can't say whether analog computers will take",0.0
00:19:46.170,00:19:48.690,"off the way digital did last century,",0.0
00:19:48.690,00:19:51.360,but they do seem to be better suited,0.5927
00:19:51.360,00:19:53.550,to a lot of the tasks that we want computers,0.0772
00:19:53.550,00:19:55.210,"to perform today,",0.0
00:19:55.210,00:19:56.240,which is a little bit funny,0.3862
00:19:56.240,00:19:58.080,because I always thought of digital,0.0
00:19:58.080,00:20:01.530,as the optimal way of processing information.,0.3612
00:20:01.530,00:20:03.770,"Everything from music to pictures,",0.0
00:20:03.770,00:20:07.860,to video has all gone digital in the last 50 years.,0.0
00:20:07.860,00:20:09.670,"But maybe in a 100 years,",0.0
00:20:09.670,00:20:11.107,"we will look back on digital,",0.0
00:20:11.107,00:20:15.060,"not not as the end point of information technology,",0.0
00:20:15.060,00:20:17.320,but as a starting point.,0.0
00:20:17.320,00:20:19.070,Our brains are digital,0.0
00:20:19.070,00:20:21.930,"in that a neuron either fires or it doesn't,",0.0
00:20:21.930,00:20:24.040,but they're also analog,0.0
00:20:24.040,00:20:28.220,"in that thinking takes place everywhere, all at once.",0.0
00:20:28.220,00:20:30.070,So maybe what we need,0.0
00:20:30.070,00:20:32.490,"to achieve true artificial intelligence,",0.7096
00:20:32.490,00:20:37.134,"machines that think like us, is the power of analog.",0.3612
00:20:37.134,00:20:39.717,(gentle music),0.4404
00:20:42.040,00:20:44.310,"Hey, I learned a lot while making this video,",0.0
00:20:44.310,00:20:47.280,much of it by playing with an actual analog computer.,0.2023
00:20:47.280,00:20:48.810,"You know, trying things out for yourself",0.0
00:20:48.810,00:20:50.300,"is really the best way to learn,",0.6682
00:20:50.300,00:20:53.470,"and you can do that with this video sponsor, Brilliant.",0.5859
00:20:53.470,00:20:54.830,Brilliant is a website and app,0.5859
00:20:54.830,00:20:56.060,that gets you thinking deeply,0.0
00:20:56.060,00:20:58.200,by engaging you in problem-solving.,0.34
00:20:58.200,00:21:00.050,"They have a great course on neural networks,",0.6249
00:21:00.050,00:21:02.460,where you can test how it works for yourself.,0.0
00:21:02.460,00:21:04.290,It gives you an excellent intuition,0.5719
00:21:04.290,00:21:07.390,"about how neural networks can recognize numbers and shapes,",0.0
00:21:07.390,00:21:09.550,and it also allows you to experience the importance,0.3612
00:21:09.550,00:21:11.850,of good training data and hidden layers,0.4404
00:21:11.850,00:21:14.040,to understand why more sophisticated,0.5984
00:21:14.040,00:21:15.860,neural networks work better.,0.4404
00:21:15.860,00:21:16.940,What I love about Brilliant,0.8402
00:21:16.940,00:21:19.320,is it tests your knowledge as you go.,0.0
00:21:19.320,00:21:20.810,"The lessons are highly interactive,",0.0
00:21:20.810,00:21:23.460,and they get progressively harder as you go on.,0.0
00:21:23.460,00:21:26.620,"And if you get stuck, there are always helpful hints.",0.2023
00:21:26.620,00:21:27.770,"For viewers of this video,",0.0
00:21:27.770,00:21:29.570,Brilliant is offering the first 200 people,0.5859
00:21:29.570,00:21:32.100,20% off an annual premium subscription.,0.0
00:21:32.100,00:21:35.210,Just go to brilliant.org/veritasium.,0.0
00:21:35.210,00:21:37.430,I will put that link down in the description.,0.0
00:21:37.430,00:21:40.030,"So I wanna thank Brilliant for supporting Veritasium,",0.8578
00:21:40.030,00:21:41.780,and I wanna thank you for watching.,0.3612
