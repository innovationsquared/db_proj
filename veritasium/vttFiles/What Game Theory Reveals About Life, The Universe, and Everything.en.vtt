WEBVTT
Kind: captions
Language: en

00:00:00.180 --> 00:00:02.580
- This is a video about
the most famous problem

00:00:02.580 --> 00:00:03.720
in game theory.

00:00:03.720 --> 00:00:07.320
Problems of this sort pop up
everywhere, from nations locked

00:00:07.320 --> 00:00:10.410
in conflict to roommates doing the dishes.

00:00:10.410 --> 00:00:13.950
Even game shows have been
based around this concept.

00:00:13.950 --> 00:00:16.230
Figuring out the best strategy
can mean the difference

00:00:16.230 --> 00:00:19.350
between life and death, war and peace,

00:00:19.350 --> 00:00:22.440
flourishing and the
destruction of the planet.

00:00:22.440 --> 00:00:24.060
And in the mechanics of this game,

00:00:24.060 --> 00:00:26.000
we may find the very source of one

00:00:26.000 --> 00:00:30.398
of the most unexpected phenomena
in nature: cooperation.

00:00:30.398 --> 00:00:32.981
(upbeat music)

00:00:34.560 --> 00:00:36.810
On the 3rd of September, 1949,

00:00:36.810 --> 00:00:38.610
an American weather monitoring plane

00:00:38.610 --> 00:00:41.100
collected air samples over Japan.

00:00:41.100 --> 00:00:45.213
In those samples, they found
traces of radioactive material.

00:00:46.110 --> 00:00:47.520
The Navy quickly collected

00:00:47.520 --> 00:00:49.890
and tested rainwater
samples from their ships

00:00:49.890 --> 00:00:52.350
and bases all over the world.

00:00:52.350 --> 00:00:53.947
They also detected small amounts

00:00:53.947 --> 00:00:57.843
of Cerium-141 and Yttrium-91.

00:00:58.740 --> 00:01:02.040
But these isotopes have half
lives of one or two months,

00:01:02.040 --> 00:01:04.560
so they must have been produced recently

00:01:04.560 --> 00:01:06.330
and the only place they
could have come from

00:01:06.330 --> 00:01:09.180
was a nuclear explosion.

00:01:09.180 --> 00:01:12.750
But the US hadn't performed
any tests that year,

00:01:12.750 --> 00:01:15.000
so the only possible conclusion

00:01:15.000 --> 00:01:17.220
was that the Soviet Union had figured out

00:01:17.220 --> 00:01:19.203
how to make a nuclear bomb.

00:01:20.280 --> 00:01:23.340
This was the news the
Americans had been dreading.

00:01:23.340 --> 00:01:25.590
Their military supremacy achieved

00:01:25.590 --> 00:01:29.010
through the Manhattan
Project was quickly fading.

00:01:29.010 --> 00:01:31.380
- This makes the problem of Western Europe

00:01:31.380 --> 00:01:35.040
and the United States far more
serious than it was before

00:01:35.040 --> 00:01:37.980
and perhaps makes the
imminence of war greater.

00:01:37.980 --> 00:01:39.870
- Some thought their
best course of action was

00:01:39.870 --> 00:01:43.320
to launch an unprovoked nuclear
strike against the Soviets

00:01:43.320 --> 00:01:45.510
while they were still ahead.

00:01:45.510 --> 00:01:47.730
In the words of Navy Secretary Matthews

00:01:47.730 --> 00:01:50.523
to become "aggressors for peace".

00:01:51.420 --> 00:01:53.970
John von Neuman, the
founder of game theory,

00:01:53.970 --> 00:01:57.420
said, "If you say why
not bomb them tomorrow,

00:01:57.420 --> 00:01:59.730
I say, why not bomb them today?

00:01:59.730 --> 00:02:01.500
If you say today at five o'clock,

00:02:01.500 --> 00:02:03.417
I say why not at one o'clock?"

00:02:06.960 --> 00:02:10.560
Something needed to be done
about nuclear weapons and fast.

00:02:10.560 --> 00:02:11.790
But what?

00:02:11.790 --> 00:02:14.130
In 1950, the RAND Corporation,

00:02:14.130 --> 00:02:17.490
a US-based think tank was
studying this question.

00:02:17.490 --> 00:02:20.500
And as part of this research,
they turned to game theory.

00:02:21.420 --> 00:02:24.540
That same year, two mathematicians
at RAND had invented

00:02:24.540 --> 00:02:27.990
a new game, one which
unbeknownst to them at the time,

00:02:27.990 --> 00:02:31.110
closely resembled the US-Soviet conflict.

00:02:31.110 --> 00:02:34.230
This game is now known as
the prisoner's dilemma.

00:02:34.230 --> 00:02:36.063
So let's play a game.

00:02:36.900 --> 00:02:39.930
A banker with a chest full
of gold coins invites you

00:02:39.930 --> 00:02:42.000
and another player to
play against each other.

00:02:42.000 --> 00:02:43.890
You each get two choices.

00:02:43.890 --> 00:02:46.800
You can cooperate or you can defect.

00:02:46.800 --> 00:02:50.310
If you both cooperate,
you each get three coins.

00:02:50.310 --> 00:02:51.690
If one of you cooperates,

00:02:51.690 --> 00:02:53.040
but the other defects,

00:02:53.040 --> 00:02:55.410
then the one who defected gets five coins

00:02:55.410 --> 00:02:57.780
and the other gets nothing.

00:02:57.780 --> 00:03:01.380
And if you both defect,
then you each get a coin.

00:03:01.380 --> 00:03:02.970
The goal of the game is simple:

00:03:02.970 --> 00:03:05.283
to get as many coins as you can.

00:03:06.960 --> 00:03:09.420
So what would you do?

00:03:09.420 --> 00:03:11.670
Suppose your opponent cooperates,

00:03:11.670 --> 00:03:15.270
then you could also
cooperate and get three coins

00:03:15.270 --> 00:03:18.810
or you could defect and
get five coins, instead.

00:03:18.810 --> 00:03:21.450
So you are better off defecting,

00:03:21.450 --> 00:03:24.750
but what if your opponent
defects, instead?

00:03:24.750 --> 00:03:27.840
Well, you could cooperate and get no coins

00:03:27.840 --> 00:03:31.770
or you could defect and
at least get one coin.

00:03:31.770 --> 00:03:34.530
So no matter what your opponent does,

00:03:34.530 --> 00:03:37.920
your best option is always to defect.

00:03:37.920 --> 00:03:40.230
Now, if your opponent is also rational,

00:03:40.230 --> 00:03:42.120
they will reach the same conclusion

00:03:42.120 --> 00:03:44.130
and therefore also defect.

00:03:44.130 --> 00:03:46.770
As a result, when you both act rationally,

00:03:46.770 --> 00:03:49.650
you both end up in the
suboptimal situation

00:03:49.650 --> 00:03:51.240
getting one coin each

00:03:51.240 --> 00:03:54.330
when you could have gotten three, instead.

00:03:54.330 --> 00:03:56.760
In the case of the US and Soviet Union,

00:03:56.760 --> 00:04:00.120
this led both countries to
develop huge nuclear arsenals

00:04:00.120 --> 00:04:03.360
of tens of thousands of
nuclear weapons each,

00:04:03.360 --> 00:04:07.260
more than enough to destroy
each other many times over.

00:04:07.260 --> 00:04:11.280
But since both countries had
nukes, neither could use them.

00:04:11.280 --> 00:04:12.690
And both countries spent

00:04:12.690 --> 00:04:16.530
around $10 trillion
developing these weapons.

00:04:16.530 --> 00:04:19.860
Both would've been better
off if they had cooperated

00:04:19.860 --> 00:04:22.860
and agreed not to develop
this technology further.

00:04:22.860 --> 00:04:25.920
But since they both acted
in their own best interest,

00:04:25.920 --> 00:04:29.733
they ended up in a situation
where everyone was worse off.

00:04:30.960 --> 00:04:32.550
The prisoner's dilemma is one

00:04:32.550 --> 00:04:34.920
of the most famous games in game theory.

00:04:34.920 --> 00:04:37.500
Thousands and thousands of
papers have been published

00:04:37.500 --> 00:04:39.390
on versions of this game.

00:04:39.390 --> 00:04:42.542
In part, that's because
it pops up everywhere.

00:04:42.542 --> 00:04:44.370
(bright music)

00:04:44.370 --> 00:04:46.950
Impalas living in
between African woodlands

00:04:46.950 --> 00:04:50.370
and Savannahs are prone to
catching ticks, which can lead

00:04:50.370 --> 00:04:53.670
to infectious diseases,
paralysis, even death.

00:04:53.670 --> 00:04:56.730
So it's important for
impalas to remove ticks

00:04:56.730 --> 00:04:58.560
and they do this by grooming,

00:04:58.560 --> 00:05:01.170
but they can't reach all
the spots on their bodies

00:05:01.170 --> 00:05:04.113
and therefore they need
another impala to groom them.

00:05:05.190 --> 00:05:08.010
Now, grooming someone
else comes at a cost.

00:05:08.010 --> 00:05:11.550
It costs saliva, electrolytes,
time and attention,

00:05:11.550 --> 00:05:14.520
all vital resources
under the hot African sun

00:05:14.520 --> 00:05:17.220
where a predator could
strike at any moment.

00:05:17.220 --> 00:05:20.250
So for the other impala,
it would be best not

00:05:20.250 --> 00:05:22.650
to pay this cost, but then again,

00:05:22.650 --> 00:05:25.050
it too will need help grooming.

00:05:25.050 --> 00:05:27.180
So all impalas face a choice:

00:05:27.180 --> 00:05:29.400
should they groom each other or not?

00:05:29.400 --> 00:05:32.403
In other words, should
they cooperate or defect?

00:05:34.050 --> 00:05:36.270
Well, if they only interact once,

00:05:36.270 --> 00:05:39.540
then the rational solution
is always to defect.

00:05:39.540 --> 00:05:43.590
That other impala is never
gonna help you, so why bother?

00:05:43.590 --> 00:05:45.240
But the thing about a lot of problems is

00:05:45.240 --> 00:05:48.030
that they're not a single
prisoner's dilemma.

00:05:48.030 --> 00:05:50.400
Impalas see each other day after day

00:05:50.400 --> 00:05:54.120
and the same situation keeps
happening over and over again.

00:05:54.120 --> 00:05:56.670
So that changes the problem

00:05:56.670 --> 00:05:59.880
because instead of playing the
prisoner's dilemma just once,

00:05:59.880 --> 00:06:02.910
you're now playing it many, many times.

00:06:02.910 --> 00:06:05.580
And if I defect now, then
my opponent will know

00:06:05.580 --> 00:06:07.620
that I'd defected and they can use

00:06:07.620 --> 00:06:09.333
that against me in the future.

00:06:10.470 --> 00:06:14.643
So what is the best strategy
in this repeated game?

00:06:18.300 --> 00:06:20.220
That is what Robert Axelrod,

00:06:20.220 --> 00:06:22.710
a political scientist wanted to find out.

00:06:22.710 --> 00:06:26.430
So in 1980 he decided to
hold a computer tournament.

00:06:26.430 --> 00:06:28.740
- He invited some of the
world's leading game theorists

00:06:28.740 --> 00:06:32.880
for many different subjects
to submit computer programs

00:06:32.880 --> 00:06:34.830
that would play each other.

00:06:34.830 --> 00:06:38.040
- Axelrod called these
programs strategies.

00:06:38.040 --> 00:06:41.160
Each strategy would face off
against every other strategy

00:06:41.160 --> 00:06:43.860
and against a copy of itself

00:06:43.860 --> 00:06:47.070
and each matchup would go for 200 rounds.

00:06:47.070 --> 00:06:49.440
That's important and
we'll come back to it.

00:06:49.440 --> 00:06:52.140
Now, Axelrod used points instead of coins,

00:06:52.140 --> 00:06:53.970
but the payoffs were the same.

00:06:53.970 --> 00:06:56.760
The goal of the tournament
was to win as many points

00:06:56.760 --> 00:06:58.890
as possible and in the end,

00:06:58.890 --> 00:07:01.740
the whole tournament was
repeated five times over

00:07:01.740 --> 00:07:06.150
to ensure the success was
robust and not just a fluke.

00:07:06.150 --> 00:07:09.330
Axelrod gave an example
of a simple strategy.

00:07:09.330 --> 00:07:12.810
It would start each game by
cooperating and only defect

00:07:12.810 --> 00:07:16.200
after its opponent had
defected twice in a row.

00:07:16.200 --> 00:07:19.230
In total Axelrod received 14 strategies

00:07:19.230 --> 00:07:21.720
and he added a 15th called random,

00:07:21.720 --> 00:07:23.940
which just randomly cooperates or defects

00:07:23.940 --> 00:07:25.593
50% of the time.

00:07:27.060 --> 00:07:29.820
All strategies were loaded
onto a single computer

00:07:29.820 --> 00:07:32.280
where they faced off against each other.

00:07:32.280 --> 00:07:34.890
One of the strategies was called Friedman.

00:07:34.890 --> 00:07:36.900
It starts off by cooperating,

00:07:36.900 --> 00:07:39.300
but if its opponent defects just once,

00:07:39.300 --> 00:07:42.660
it will keep defecting for
the remainder of the game.

00:07:42.660 --> 00:07:45.000
Another strategy was called Joss.

00:07:45.000 --> 00:07:47.010
It also starts by cooperating,

00:07:47.010 --> 00:07:48.240
but then it just copies

00:07:48.240 --> 00:07:50.850
what the other player
did on the last move.

00:07:50.850 --> 00:07:55.410
Then around 10% of the time,
Joss gets sneaky and defects.

00:07:55.410 --> 00:07:57.600
There was also a rather elaborate strategy

00:07:57.600 --> 00:07:58.770
called Graaskamp.

00:07:58.770 --> 00:08:00.720
This strategy works the same as Joss,

00:08:00.720 --> 00:08:02.850
but instead of defecting
probabilistically,

00:08:02.850 --> 00:08:05.280
Graaskamp defects in the 50th round

00:08:05.280 --> 00:08:07.860
to try and probe the
strategy of its opponent

00:08:07.860 --> 00:08:10.740
and see if it can take
advantage of any weaknesses.

00:08:10.740 --> 00:08:13.860
The most elaborate
strategy was Name Withheld

00:08:13.860 --> 00:08:16.143
with 77 lines of code.

00:08:17.160 --> 00:08:19.890
After all the games were played,
the results were tallied up

00:08:19.890 --> 00:08:21.780
and the leaderboard established.

00:08:21.780 --> 00:08:23.160
- The crazy thing was

00:08:23.160 --> 00:08:25.680
that the simplest
program ended up winning,

00:08:25.680 --> 00:08:28.770
a program that came to
be called Tit for Tat.

00:08:28.770 --> 00:08:31.170
- [Derek] Tit for Tat
starts by cooperating

00:08:31.170 --> 00:08:32.670
and then it copies exactly

00:08:32.670 --> 00:08:35.130
what its opponent did in the last move.

00:08:35.130 --> 00:08:37.950
So it would follow
cooperation with cooperation

00:08:37.950 --> 00:08:40.110
and defection with defection,

00:08:40.110 --> 00:08:43.110
but only once if it's opponent
goes back to cooperating.

00:08:43.110 --> 00:08:44.643
So does Tit for Tat.

00:08:45.570 --> 00:08:47.670
When Tit for Tat played against Friedman,

00:08:47.670 --> 00:08:50.910
both started by cooperating
and they kept cooperating

00:08:50.910 --> 00:08:54.540
both ending up with perfect
scores for complete cooperation.

00:08:54.540 --> 00:08:56.400
When Tit for Tat played against Joss,

00:08:56.400 --> 00:08:58.560
they too started by cooperating

00:08:58.560 --> 00:09:01.200
but then on the sixth move, Joss defected.

00:09:01.200 --> 00:09:03.930
This sparked a series of
back and forth defections,

00:09:03.930 --> 00:09:05.700
a sort of echo effect.

00:09:05.700 --> 00:09:08.250
- Okay, so now you've got
this alternating thing

00:09:08.250 --> 00:09:10.230
which will remind you
of some of the politics

00:09:10.230 --> 00:09:12.570
of the world today where we
have to do something to you

00:09:12.570 --> 00:09:14.130
because of what you did to us.

00:09:14.130 --> 00:09:16.530
And then when this weird program throws

00:09:16.530 --> 00:09:20.640
in a second unprovoked
defection, now it's really bad

00:09:20.640 --> 00:09:23.760
because now both programs are
gonna defect on each other

00:09:23.760 --> 00:09:25.500
for the rest of the game.

00:09:25.500 --> 00:09:27.360
And that's also like some of the things

00:09:27.360 --> 00:09:29.010
that we're seeing in politics today

00:09:29.010 --> 00:09:31.110
and in international relations.

00:09:31.110 --> 00:09:33.660
- As a result of these
mutual retaliations,

00:09:33.660 --> 00:09:36.750
both Tit for Tat and Joss did poorly.

00:09:36.750 --> 00:09:39.390
But because Tit for Tat
managed to cooperate

00:09:39.390 --> 00:09:41.010
with enough other strategies,

00:09:41.010 --> 00:09:41.843
it still won the tournament.

00:09:43.043 --> 00:09:44.368
- As we are being joined...

00:09:44.368 --> 00:09:46.260
- Hey my God, there's Professor Axelrod.

00:09:46.260 --> 00:09:48.603
- Hey, there's Steven Strogatz.

00:09:50.280 --> 00:09:52.740
- Whoa, what a treat this is.

00:09:52.740 --> 00:09:55.050
- And I imagine initially it'd be sort

00:09:55.050 --> 00:09:55.980
of like computer chess

00:09:55.980 --> 00:09:57.780
where you need a pretty
complicated program

00:09:57.780 --> 00:09:59.760
to play a sophisticated game.

00:09:59.760 --> 00:10:01.830
But in fact it was not like that at all.

00:10:01.830 --> 00:10:04.830
It was the simplest
strategy that did the best.

00:10:04.830 --> 00:10:08.190
So I analyzed how that happened.

00:10:08.190 --> 00:10:10.920
- Axelrod found that all the
best performing strategies,

00:10:10.920 --> 00:10:14.343
including Tit for Tat,
shared four qualities.

00:10:15.210 --> 00:10:17.400
First, they were all nice,

00:10:17.400 --> 00:10:20.220
which just means they are
not the first to defect.

00:10:20.220 --> 00:10:22.290
So Tit for Tat is a nice strategy,

00:10:22.290 --> 00:10:25.860
it can defect but only in retaliation.

00:10:25.860 --> 00:10:28.230
The opposite of nice is nasty.

00:10:28.230 --> 00:10:30.330
That's a strategy that defects first.

00:10:30.330 --> 00:10:32.670
So Joss is nasty.

00:10:32.670 --> 00:10:34.680
Outta the 15 strategies in the tournament,

00:10:34.680 --> 00:10:37.470
eight were nice and seven nasty.

00:10:37.470 --> 00:10:40.710
The top eight strategies were all nice

00:10:40.710 --> 00:10:42.900
and even the worst
performing nice strategy

00:10:42.900 --> 00:10:46.890
still far outscored the
best performing nasty one.

00:10:46.890 --> 00:10:50.250
The second important
quality was being forgiving.

00:10:50.250 --> 00:10:52.980
A forgiving strategy is
one that can retaliate

00:10:52.980 --> 00:10:54.750
but it doesn't hold a grudge.

00:10:54.750 --> 00:10:57.360
So Tit for Tat is a forgiving strategy.

00:10:57.360 --> 00:10:59.790
It retaliates when its opponent defects

00:10:59.790 --> 00:11:01.080
but it doesn't let affections

00:11:01.080 --> 00:11:05.010
from before the last round
influence its current decisions.

00:11:05.010 --> 00:11:08.760
Friedman on the other hand,
is maximally unforgiving

00:11:08.760 --> 00:11:12.180
- After the first defection
just from the opponent

00:11:12.180 --> 00:11:14.250
would defect for the rest of the game.

00:11:14.250 --> 00:11:15.570
Okay, that's it.

00:11:15.570 --> 00:11:19.410
No mercy and that might feel good to do

00:11:19.410 --> 00:11:23.280
but it doesn't end up working
out well in the long run.

00:11:23.280 --> 00:11:25.620
- This conclusion that it pays to be nice

00:11:25.620 --> 00:11:28.860
and forgiving came as
a shock to the experts.

00:11:28.860 --> 00:11:30.150
Many had tried to be tricky

00:11:30.150 --> 00:11:32.970
and create subtle nasty
strategies to beat their opponent

00:11:32.970 --> 00:11:35.160
and eke out an advantage,

00:11:35.160 --> 00:11:36.480
but they all failed.

00:11:36.480 --> 00:11:38.010
Instead, in this tournament,

00:11:38.010 --> 00:11:40.170
nice guys finished first.

00:11:40.170 --> 00:11:41.940
Now Tit for Tat is quite forgiving

00:11:41.940 --> 00:11:44.790
but it's possible to
be even more forgiving.

00:11:44.790 --> 00:11:47.160
Axelrod's sample strategy only defects

00:11:47.160 --> 00:11:49.770
after its opponent
defected twice in a row.

00:11:49.770 --> 00:11:51.660
It was Tit for Two Tats.

00:11:51.660 --> 00:11:54.270
Now that might sound overly generous,

00:11:54.270 --> 00:11:56.100
but when Axelrod ran the numbers,

00:11:56.100 --> 00:11:59.490
he found that if anyone had
submitted the Sample strategy,

00:11:59.490 --> 00:12:01.653
they would've won the tournament.

00:12:05.250 --> 00:12:06.090
- I mean it's so clever,

00:12:06.090 --> 00:12:07.940
there's so many layers to this story.

00:12:09.000 --> 00:12:11.340
After Axelrod published his analysis

00:12:11.340 --> 00:12:13.140
of what happened or circulated it

00:12:13.140 --> 00:12:15.180
among these game theorists, he said,

00:12:15.180 --> 00:12:20.180
now that we all know what
worked well, let's try again.

00:12:20.550 --> 00:12:22.590
- So he announced a second tournament

00:12:22.590 --> 00:12:24.660
where everything would be the same except

00:12:24.660 --> 00:12:28.350
for one change the number
of rounds per game.

00:12:28.350 --> 00:12:29.670
See in the first tournament,

00:12:29.670 --> 00:12:32.820
each game lasted precisely 200 rounds.

00:12:32.820 --> 00:12:34.860
And that is important because if you know

00:12:34.860 --> 00:12:36.210
when the last round is,

00:12:36.210 --> 00:12:39.360
then there's no reason to
cooperate in that round.

00:12:39.360 --> 00:12:41.190
So you're better off defecting.

00:12:41.190 --> 00:12:43.290
Of course your opponent
should reason the same

00:12:43.290 --> 00:12:46.200
and so they should also
defect in the last round.

00:12:46.200 --> 00:12:49.710
But if you both anticipate
defection in the last round,

00:12:49.710 --> 00:12:51.990
then there's no reason for
you to cooperate in the second

00:12:51.990 --> 00:12:54.900
to last round or the round
before that, or before that

00:12:54.900 --> 00:12:57.780
and so on all the way
to the very first round.

00:12:57.780 --> 00:13:00.000
- And so in Axelrod's tournament,

00:13:00.000 --> 00:13:01.290
it was a very important thing

00:13:01.290 --> 00:13:03.060
that the players didn't know exactly

00:13:03.060 --> 00:13:04.980
how long they were gonna be playing.

00:13:04.980 --> 00:13:07.860
They knew on average
it would be 200 rounds,

00:13:07.860 --> 00:13:10.320
but there was a random number generator

00:13:10.320 --> 00:13:13.260
that prevented them from
knowing with certainty.

00:13:13.260 --> 00:13:15.510
- Yeah, if you're not sure when it ends,

00:13:15.510 --> 00:13:17.040
then you have to kind of keep cooperating

00:13:17.040 --> 00:13:18.792
'cause it might keep going
and you need might need them

00:13:18.792 --> 00:13:21.540
on your side.
- That's right.

00:13:21.540 --> 00:13:25.320
- For this second tournament,
Axelrod received 62 entries

00:13:25.320 --> 00:13:27.480
and again, added random.

00:13:27.480 --> 00:13:29.190
The contestants had gotten the results

00:13:29.190 --> 00:13:30.870
and analysis from the first tournament

00:13:30.870 --> 00:13:33.690
and could use this information
to their advantage.

00:13:33.690 --> 00:13:35.880
This created two camps.

00:13:35.880 --> 00:13:38.010
Some thought that clearly being nice

00:13:38.010 --> 00:13:39.900
and forgiving were winning qualities.

00:13:39.900 --> 00:13:42.870
So they submitted nice
and forgiving strategies.

00:13:42.870 --> 00:13:45.360
One even submitted Tit for Two Tats.

00:13:45.360 --> 00:13:48.450
The second camp anticipated
that others would be nice

00:13:48.450 --> 00:13:50.070
and extra forgiving

00:13:50.070 --> 00:13:52.260
and therefore they
submitted nasty strategies

00:13:52.260 --> 00:13:56.220
to try to take advantage of
those that were extra forgiving.

00:13:56.220 --> 00:13:58.650
One such strategy was called Tester.

00:13:58.650 --> 00:14:00.150
It would defect on the first move

00:14:00.150 --> 00:14:02.250
to see how its opponent reacted.

00:14:02.250 --> 00:14:04.860
If it retaliated, tester would apologize

00:14:04.860 --> 00:14:07.140
and play Tit for Tat for
the remainder of the game.

00:14:07.140 --> 00:14:08.490
If it didn't retaliate,

00:14:08.490 --> 00:14:11.133
tester would defect every
other move after that.

00:14:13.770 --> 00:14:16.800
But again, being nasty didn't pay.

00:14:16.800 --> 00:14:21.330
- And once again, Tit for
Tat was the most effective.

00:14:21.330 --> 00:14:24.120
- Nice strategies again did much better.

00:14:24.120 --> 00:14:27.450
In the top 15, only one was not nice.

00:14:27.450 --> 00:14:31.893
Similarly, in the bottom
15, only one was not nasty.

00:14:32.940 --> 00:14:34.290
After the second tournament,

00:14:34.290 --> 00:14:36.390
Axelrod identified the other qualities

00:14:36.390 --> 00:14:38.880
that distinguished the
better performing strategies.

00:14:38.880 --> 00:14:41.340
The third is being retaliatory,

00:14:41.340 --> 00:14:43.500
which means if your opponent defects,

00:14:43.500 --> 00:14:47.430
strike back immediately,
don't be a pushover.

00:14:47.430 --> 00:14:50.070
Always cooperate is a total pushover.

00:14:50.070 --> 00:14:52.560
And so it's very easy
to take advantage of.

00:14:52.560 --> 00:14:54.000
Tit for Tat, on the other hand,

00:14:54.000 --> 00:14:56.283
is very hard to take advantage of.

00:14:57.120 --> 00:15:01.020
The last quality that Axelrod
identified is being clear.

00:15:01.020 --> 00:15:03.360
- Programs that were too opaque,

00:15:03.360 --> 00:15:06.660
that were too similar to a random program,

00:15:06.660 --> 00:15:07.680
you couldn't figure them out

00:15:07.680 --> 00:15:09.600
because they were so complicated,

00:15:09.600 --> 00:15:12.030
tt was very hard to establish
any pattern of trust

00:15:12.030 --> 00:15:13.170
with a program like that

00:15:13.170 --> 00:15:16.238
because you couldn't figure
out what it was doing.

00:15:16.238 --> 00:15:17.071
Not you.

00:15:17.071 --> 00:15:18.960
I mean the other programs it was playing

00:15:18.960 --> 00:15:21.510
couldn't figure them out and
so they would end up more

00:15:21.510 --> 00:15:24.510
or less defaulting to thinking
every turn is like the

00:15:24.510 --> 00:15:25.860
last time I'm gonna see you.

00:15:25.860 --> 00:15:27.900
So I might as well defect.

00:15:27.900 --> 00:15:31.590
What to me is mind blowing about this

00:15:31.590 --> 00:15:36.330
is that these four principles
being nice, forgiving,

00:15:36.330 --> 00:15:40.200
provokable and clear is
a lot like the morality

00:15:40.200 --> 00:15:44.040
that has evolved around the
world that is often summarized

00:15:44.040 --> 00:15:45.213
as an eye for an eye.

00:15:46.230 --> 00:15:48.630
It's not Christianity, by the way.

00:15:48.630 --> 00:15:51.510
It's not to not turn the
other cheek philosophy,

00:15:51.510 --> 00:15:54.153
it's some older philosophy.

00:15:57.150 --> 00:15:58.200
- What's interesting is that

00:15:58.200 --> 00:16:00.690
while Tit for Two Tats would've
won the first tournament,

00:16:00.690 --> 00:16:03.900
it only came 24th in
the second tournament.

00:16:03.900 --> 00:16:06.060
This highlights an important fact:

00:16:06.060 --> 00:16:07.920
in the repeated prisoner's dilemma,

00:16:07.920 --> 00:16:10.650
there is no single best strategy.

00:16:10.650 --> 00:16:13.320
The strategy that performs
best always depends

00:16:13.320 --> 00:16:15.960
on the other strategies
it's interacting with.

00:16:15.960 --> 00:16:18.390
For example, if you put Tit
for Tat in an environment

00:16:18.390 --> 00:16:21.720
with only the ultimate
bullies of always defect,

00:16:21.720 --> 00:16:24.033
then Tit for Tat comes in last.

00:16:24.870 --> 00:16:27.060
- I wanted to see whether, for example,

00:16:27.060 --> 00:16:28.680
the Tit for Tat did well

00:16:28.680 --> 00:16:31.470
because it did well
with really stupid rules

00:16:31.470 --> 00:16:33.060
that didn't do well with it all themselves

00:16:33.060 --> 00:16:35.160
that basically it took
advantage of people.

00:16:35.160 --> 00:16:36.480
- So he ran a simulation

00:16:36.480 --> 00:16:38.940
where successful strategies
in one generation

00:16:38.940 --> 00:16:41.790
would see their numbers
grow and unsuccessful ones

00:16:41.790 --> 00:16:43.680
would see their numbers drop.

00:16:43.680 --> 00:16:44.790
In this simulation,

00:16:44.790 --> 00:16:47.070
the worst performing
strategies quickly shrink

00:16:47.070 --> 00:16:49.830
and go extinct, while the
top performing strategies

00:16:49.830 --> 00:16:51.450
become more common.

00:16:51.450 --> 00:16:54.930
Harrington, the only nasty
strategy in the top 15,

00:16:54.930 --> 00:16:56.700
first grew quickly,

00:16:56.700 --> 00:16:59.820
but then as the strategies it
was preying on went extinct,

00:16:59.820 --> 00:17:02.553
Harrington's numbers also quickly dropped.

00:17:03.930 --> 00:17:06.510
This shows a main benefit
of this simulation

00:17:06.510 --> 00:17:09.150
because it tests how well a strategy does

00:17:09.150 --> 00:17:11.790
with other successful strategies.

00:17:11.790 --> 00:17:13.200
After a thousand generations,

00:17:13.200 --> 00:17:15.720
the proportions are mostly stable

00:17:15.720 --> 00:17:18.390
and only nice strategies survive.

00:17:18.390 --> 00:17:21.180
Again, Tit for Tat comes out on top,

00:17:21.180 --> 00:17:25.380
representing 14.5% of
the total population.

00:17:25.380 --> 00:17:28.290
Now this process may sound
similar to evolution,

00:17:28.290 --> 00:17:29.970
but there is a subtle difference,

00:17:29.970 --> 00:17:32.520
which is that in this case
there are no mutations.

00:17:32.520 --> 00:17:35.880
So it's actually an ecological simulation.

00:17:35.880 --> 00:17:39.330
But what if the world you
started in was different?

00:17:39.330 --> 00:17:42.240
- Imagine a world that is a
really nasty place to live,

00:17:42.240 --> 00:17:45.060
more or less populated with
players that always defect,

00:17:45.060 --> 00:17:48.150
except there's a little
cluster of tit-for-tat players

00:17:48.150 --> 00:17:50.250
that live in some kind of nucleus

00:17:50.250 --> 00:17:51.870
and they get to play with each other a lot

00:17:51.870 --> 00:17:54.720
because they're
geographically sequestered.

00:17:54.720 --> 00:17:57.000
They will start building
up a lot of points,

00:17:57.000 --> 00:17:59.850
and also because that
translates into offspring,

00:17:59.850 --> 00:18:02.130
they'll start to take over the population.

00:18:02.130 --> 00:18:05.190
So in fact, Axelrod showed
that a little island

00:18:05.190 --> 00:18:09.480
of cooperation can emerge and spread

00:18:09.480 --> 00:18:11.850
and eventually will take over the world,

00:18:11.850 --> 00:18:13.443
which is fantastic.

00:18:14.682 --> 00:18:19.682
How can cooperation emerge in a population

00:18:19.682 --> 00:18:21.990
of players who are self-interested?

00:18:21.990 --> 00:18:25.320
Who are not trying to be good
because they're good-hearted.

00:18:25.320 --> 00:18:28.140
You don't have to be altruistic.

00:18:28.140 --> 00:18:30.720
You could be looking out
for number one for yourself

00:18:30.720 --> 00:18:31.860
and your own interests.

00:18:31.860 --> 00:18:34.746
And yet cooperation can still emerge.

00:18:34.746 --> 00:18:38.040
(bright music)

00:18:38.040 --> 00:18:39.960
- Some argue that this
could explain how we went

00:18:39.960 --> 00:18:43.060
from a world full of
completely selfish organisms

00:18:45.060 --> 00:18:47.730
where every organism only
cared about themselves

00:18:47.730 --> 00:18:51.660
to one where cooperation
emerged and flourished.

00:18:51.660 --> 00:18:53.460
From impalas grooming each other

00:18:53.460 --> 00:18:55.503
to fish cleaning sharks.

00:18:56.520 --> 00:18:59.280
Many life forms experience
conflicts similar

00:18:59.280 --> 00:19:01.080
to the prisoner's dilemma,

00:19:01.080 --> 00:19:03.213
but because they don't interact just once,

00:19:04.320 --> 00:19:06.681
both can be better off by cooperating.

00:19:08.130 --> 00:19:11.520
And this doesn't require trust
or conscious thought either

00:19:11.520 --> 00:19:15.690
because the strategy
could be encoded in DNA,

00:19:15.690 --> 00:19:18.450
as long as it performs better
than the other strategies,

00:19:18.450 --> 00:19:20.583
it can take over a population.

00:19:27.030 --> 00:19:28.620
Axelrod's insights were applied

00:19:28.620 --> 00:19:30.720
to areas like evolutionary biology

00:19:30.720 --> 00:19:32.640
and international conflicts,

00:19:32.640 --> 00:19:34.320
but there was one aspect

00:19:34.320 --> 00:19:36.480
that his original
tournaments didn't cover.

00:19:36.480 --> 00:19:38.400
What happens if there is a little bit

00:19:38.400 --> 00:19:40.710
of random error in the game?

00:19:40.710 --> 00:19:42.630
Some noise in the system.

00:19:42.630 --> 00:19:44.850
For example, one player
tries to cooperate,

00:19:44.850 --> 00:19:47.010
but it comes across as a defection.

00:19:47.010 --> 00:19:48.450
Little errors like this happen

00:19:48.450 --> 00:19:50.490
in the real world all the time.

00:19:50.490 --> 00:19:52.080
Like in 1983,

00:19:52.080 --> 00:19:54.210
the Soviet satellite-based
early warning system

00:19:54.210 --> 00:19:56.970
detected the launch of an
intercontinental ballistic missile

00:19:56.970 --> 00:20:00.960
from the US but the US
hadn't launched anything.

00:20:00.960 --> 00:20:03.960
The Soviet system had confused
sunlight reflecting off

00:20:03.960 --> 00:20:07.440
high altitude clouds
with a ballistic missile.

00:20:07.440 --> 00:20:09.180
Thankfully, Stanislav Petrov,

00:20:09.180 --> 00:20:12.390
the Soviet officer on
duty, dismissed the alarm.

00:20:12.390 --> 00:20:15.930
But this example shows the
potential costs of a signal error

00:20:15.930 --> 00:20:18.450
and the importance of
studying the effects of noise

00:20:18.450 --> 00:20:20.340
on those strategies.

00:20:20.340 --> 00:20:22.890
- The word game sounds
like it's a children's game

00:20:22.890 --> 00:20:24.480
or, you know, there's some something,

00:20:24.480 --> 00:20:27.360
a misnomer maybe in calling it game theory

00:20:27.360 --> 00:20:28.620
because this is,

00:20:28.620 --> 00:20:31.620
these are life and
death matters obviously.

00:20:31.620 --> 00:20:33.900
And as you mentioned that
this came up in the Cold War.

00:20:33.900 --> 00:20:36.180
I mean it could actually be life and death

00:20:36.180 --> 00:20:37.110
of the whole planet,

00:20:37.110 --> 00:20:39.720
the whole we could annihilate
human civilization.

00:20:39.720 --> 00:20:43.440
So these are not games in
any kind of trivial sense,

00:20:43.440 --> 00:20:44.910
it's just the term that is used

00:20:44.910 --> 00:20:47.070
by mathematicians and theorists.

00:20:47.070 --> 00:20:49.110
- When Tit for Tat plays against itself

00:20:49.110 --> 00:20:50.610
in a noisy environment,

00:20:50.610 --> 00:20:53.100
both start off by cooperating,

00:20:53.100 --> 00:20:56.730
but if a single cooperation
is perceived as a defection,

00:20:56.730 --> 00:20:58.590
then the other Tit for Tat retaliates

00:20:58.590 --> 00:21:02.340
and it sets off a chain of
alternating retaliations.

00:21:02.340 --> 00:21:05.010
And if another cooperation
is perceived as a defection,

00:21:05.010 --> 00:21:08.730
then the rest of the game is
constant mutual defection.

00:21:08.730 --> 00:21:11.370
Therefore, in the long run,
both would only get a third

00:21:11.370 --> 00:21:14.490
of the points they would get
in a perfect environment.

00:21:14.490 --> 00:21:16.980
Tit for Tat goes from performing very well

00:21:16.980 --> 00:21:19.080
to performing poorly.

00:21:19.080 --> 00:21:21.540
So how do you solve this?

00:21:21.540 --> 00:21:23.040
Well, you need a reliable way

00:21:23.040 --> 00:21:25.230
to break out of these echo effects.

00:21:25.230 --> 00:21:28.080
And one way to do this is
by playing Tit for Tat,

00:21:28.080 --> 00:21:31.560
but with around 10% more forgiveness.

00:21:31.560 --> 00:21:34.590
So instead of retaliating
after every defection,

00:21:34.590 --> 00:21:38.220
you only retaliate around
nine out of every 10 times.

00:21:38.220 --> 00:21:40.110
This helps you break out of those echoes

00:21:40.110 --> 00:21:41.940
while still being retaliatory enough

00:21:41.940 --> 00:21:44.520
to not be taken advantage of.

00:21:44.520 --> 00:21:46.290
- And so we also ran the tournament

00:21:46.290 --> 00:21:51.273
with noise and generosity
and that did quite well.

00:21:57.540 --> 00:22:01.380
My favorite example is Tit
for Tat does really well,

00:22:01.380 --> 00:22:02.490
but it could never do better

00:22:02.490 --> 00:22:04.190
than the player it's playing with.

00:22:05.220 --> 00:22:07.320
- I mean, think about it, by design,

00:22:07.320 --> 00:22:10.200
all they can do is lose or draw.

00:22:10.200 --> 00:22:13.560
And yet when the results of all
interactions are tallied up,

00:22:13.560 --> 00:22:17.010
they come out ahead of
all other strategies.

00:22:17.010 --> 00:22:20.640
Similarly, always defect
can never lose a game.

00:22:20.640 --> 00:22:23.010
It can only draw or win,

00:22:23.010 --> 00:22:26.073
but overall, it performs extremely poorly.

00:22:27.120 --> 00:22:29.580
This highlights a common misconception

00:22:29.580 --> 00:22:31.800
because for many people when
they think about winning,

00:22:31.800 --> 00:22:34.560
they think they need to
beat the other person.

00:22:34.560 --> 00:22:37.620
In games like chess or poker, this is true

00:22:37.620 --> 00:22:38.850
since one person's gain

00:22:38.850 --> 00:22:41.340
is necessarily another person's loss,

00:22:41.340 --> 00:22:43.680
so these games are zero sum.

00:22:43.680 --> 00:22:46.350
But most of life is not zero sum.

00:22:46.350 --> 00:22:48.360
To win, you don't need to get your reward

00:22:48.360 --> 00:22:49.530
from the other player.

00:22:49.530 --> 00:22:51.720
Instead, you can get it from the banker.

00:22:51.720 --> 00:22:54.540
Only in real life, the
banker is the world.

00:22:54.540 --> 00:22:57.510
It is literally everything around you.

00:22:57.510 --> 00:23:00.600
It is just up to us to find
those win-win situations,

00:23:00.600 --> 00:23:03.723
and then work together
to unlock those rewards.

00:23:04.650 --> 00:23:08.100
Cooperation pays even among rivals.

00:23:08.100 --> 00:23:10.020
From 1950 to 1986,

00:23:10.020 --> 00:23:12.900
the US and Soviet Union
had trouble cooperating

00:23:12.900 --> 00:23:15.330
and both kept developing nukes.

00:23:15.330 --> 00:23:17.910
But then from the late '80s onwards,

00:23:17.910 --> 00:23:21.240
they started reducing
their nuclear stockpiles.

00:23:21.240 --> 00:23:24.060
They too had learned
how to resolve conflict.

00:23:24.060 --> 00:23:25.410
Rather than making an agreement

00:23:25.410 --> 00:23:27.540
to abolish all nuclear arms at once

00:23:27.540 --> 00:23:31.440
and essentially turning it into
a single prisoner's dilemma,

00:23:31.440 --> 00:23:35.400
they would disarm slowly, a
small number of nukes each year

00:23:35.400 --> 00:23:36.840
and then they'd check each other

00:23:36.840 --> 00:23:39.450
to see that they had both cooperated

00:23:39.450 --> 00:23:41.400
and then repeat the year after,

00:23:41.400 --> 00:23:43.320
and the year after that.

00:23:43.320 --> 00:23:47.190
All along, checking to
ensure mutual cooperation.

00:23:47.190 --> 00:23:50.010
In the more than 40 years
since Axelrod's tournaments,

00:23:50.010 --> 00:23:51.270
researchers have continued

00:23:51.270 --> 00:23:53.460
to study which strategies perform best

00:23:53.460 --> 00:23:55.410
in a variety of environments.

00:23:55.410 --> 00:23:58.650
In doing so, they varied
everything from payoff structures

00:23:58.650 --> 00:24:01.680
to strategies to errors and more.

00:24:01.680 --> 00:24:05.040
Some even allowed the strategies to mutate

00:24:05.040 --> 00:24:07.110
while Tit for Tat or generous Tit for Tat

00:24:07.110 --> 00:24:09.270
doesn't always come out on top,

00:24:09.270 --> 00:24:13.980
Axelrod's main takeaways still
hold: be nice, forgiving,

00:24:13.980 --> 00:24:16.350
but don't be a pushover.

00:24:16.350 --> 00:24:19.710
- Can I ask you, why did Anatol Rapoport

00:24:19.710 --> 00:24:21.303
submit Tit for Tat?

00:24:23.640 --> 00:24:26.729
- Well, the reason was
because I asked him to.

00:24:26.729 --> 00:24:28.860
(both laugh)

00:24:28.860 --> 00:24:32.520
And he wrote saying, yeah,
that I'm willing to do that,

00:24:32.520 --> 00:24:35.280
but I just wanna be
clear that I'm not sure

00:24:35.280 --> 00:24:36.960
that this is really such a good idea.

00:24:36.960 --> 00:24:39.450
I don't, he was a peace researcher

00:24:39.450 --> 00:24:42.870
and I think his own inclinations were

00:24:42.870 --> 00:24:47.870
to be much more forgiving and
maybe not be so provokable.

00:24:49.230 --> 00:24:52.140
- What I find fascinating is
that one of the main things

00:24:52.140 --> 00:24:54.420
that sets life apart
from non-living things

00:24:54.420 --> 00:24:56.490
that life gets to make decisions.

00:24:56.490 --> 00:24:57.802
We get to make choices.

00:24:57.802 --> 00:25:00.090
Choices that don't only change our future,

00:25:00.090 --> 00:25:02.310
but also the future of
those we interact with.

00:25:02.310 --> 00:25:05.310
You see, in the short term,
it is often the environment

00:25:05.310 --> 00:25:08.217
that shapes the player that
determines who does well.

00:25:08.217 --> 00:25:11.010
But in the long run, it is the players

00:25:11.010 --> 00:25:12.993
that shape the environment.

00:25:14.130 --> 00:25:17.670
So let's play a game, the game of life,

00:25:17.670 --> 00:25:19.380
and make your choices wisely

00:25:19.380 --> 00:25:22.773
because their impact may
reach further than you think.

00:25:27.870 --> 00:25:30.270
Using the right strategy matters,

00:25:30.270 --> 00:25:33.120
but figuring out the
best strategy isn't easy.

00:25:33.120 --> 00:25:34.500
It requires critical thinking

00:25:34.500 --> 00:25:37.920
and innovative solutions
like Axelrod's tournaments.

00:25:37.920 --> 00:25:39.300
If you are looking for an easy way

00:25:39.300 --> 00:25:40.950
to build your problem solving skills,

00:25:40.950 --> 00:25:43.800
then check out this
video's sponsor, Brilliant.

00:25:43.800 --> 00:25:45.930
Brilliant will help make
you a better thinker

00:25:45.930 --> 00:25:48.090
in everything from math and data science

00:25:48.090 --> 00:25:50.070
to programming, technology,

00:25:50.070 --> 00:25:51.270
you name it.

00:25:51.270 --> 00:25:53.460
You can start right now for free,

00:25:53.460 --> 00:25:54.990
straight from your device,

00:25:54.990 --> 00:25:56.340
the device you're watching this on.

00:25:56.340 --> 00:25:58.770
All you need to do is
set your learning goal

00:25:58.770 --> 00:26:01.260
and Brilliant will design
the perfect path for you,

00:26:01.260 --> 00:26:04.080
equipping you with all the
tools you need to reach it.

00:26:04.080 --> 00:26:06.420
Did you like today's
insights from game theory

00:26:06.420 --> 00:26:08.100
then Brilliant's brand new course,

00:26:08.100 --> 00:26:11.130
Intro to Probability is
the perfect next step.

00:26:11.130 --> 00:26:13.080
Introduction to
Probability is your gateway

00:26:13.080 --> 00:26:16.560
to mastering the tools of
chance, risk, and prediction.

00:26:16.560 --> 00:26:17.820
You'll learn how to construct

00:26:17.820 --> 00:26:21.210
and analyze models of
real world situations

00:26:21.210 --> 00:26:23.970
from electrons and business
decisions all the way

00:26:23.970 --> 00:26:26.490
to the 2023 Women's World Cup.

00:26:26.490 --> 00:26:28.230
Whether you are a budding statistician

00:26:28.230 --> 00:26:31.110
or you just wanna learn
about randomness and chance,

00:26:31.110 --> 00:26:33.600
this course will equip you
with the skills you need

00:26:33.600 --> 00:26:36.420
to make decisions in uncertain situations.

00:26:36.420 --> 00:26:38.940
You'll even take a page
out of Axelrod's Playbook

00:26:38.940 --> 00:26:41.010
and learn how to build
computer simulations

00:26:41.010 --> 00:26:43.200
to put your strategies to the test.

00:26:43.200 --> 00:26:45.750
Beyond probability, Brilliant
has a massive library

00:26:45.750 --> 00:26:48.990
of content covering everything
from math to data science

00:26:48.990 --> 00:26:51.180
and programming to technology.

00:26:51.180 --> 00:26:54.210
What I love about Brilliant is
that each lesson is hands-on,

00:26:54.210 --> 00:26:56.340
so you'll build real intuition

00:26:56.340 --> 00:26:57.960
and the best part is that you can learn

00:26:57.960 --> 00:26:59.730
with brilliant on the go.

00:26:59.730 --> 00:27:02.160
To try everything Brilliant
has to offer for free

00:27:02.160 --> 00:27:06.270
for a full 30 days visit
brilliant.org/veritasium

00:27:06.270 --> 00:27:08.070
and the first 200 of you to sign up

00:27:08.070 --> 00:27:11.910
will get 20% off Brilliant's
annual premium subscription.

00:27:11.910 --> 00:27:14.520
So I want to thank Brilliant
for sponsoring this video

00:27:14.520 --> 00:27:16.473
and I want to thank you for watching.

